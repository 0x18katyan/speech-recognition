{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a05c6a0-a5b7-459d-95cb-810afe750ed3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_packed_sequence, pack_padded_sequence, PackedSequence\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CommonVoice\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_waveform, play_audio\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau, StepLR\n",
    "from torch.optim import RAdam\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, PackedSequence\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "from src.model.model import *\n",
    "\n",
    "from src.utils.dataset import CommonVoice\n",
    "from src.utils.audio_utils import plot_waveform, play_audio\n",
    "from src.utils.collate import Collator\n",
    "from src.utils.tokenizer import get_tokenizer\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bccc5281-1c6f-490f-b2de-72d7c9187fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f237251-1e2f-402f-a0b0-995c07bf1b70",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_summary, get_writer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrad_flow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from src.utils.misc import get_summary, get_writer\n",
    "from src.utils.grad_flow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2446aac2-89a2-4747-bb36-41fedc0ef545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pkbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6071366-8bd4-490f-800d-64e0439b3dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0 \n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110dab0c-10b1-4665-8726-cc37bcfd6371",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc2acde-600e-4545-9207-76b48e5abe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'data/external/cv-corpus-8.0-2022-01-19/en/'\n",
    "\n",
    "tokenizer_file = 'data/tokenizer/trained_tokenizer.json'\n",
    "\n",
    "# trimmed_train_path = 'data/internal/sample_train.tsv'\n",
    "trimmed_train_path = 'data/internal/train_trimmed.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "277f89b6-6c88-4763-a927-e2f8b8d1b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(tokenizer_file_path=tokenizer_file)\n",
    "\n",
    "blank_token_id = tokenizer.vocab[\"[BLANK]\"]\n",
    "bos_token_id = tokenizer.vocab[\"[BOS]\"]\n",
    "eos_token_id = tokenizer.vocab[\"[EOS]\"]\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4230b8ef-817a-4449-a113-19dc2518c9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    CommonVoice Dataset\n",
      "    -------------------\n",
      "    \n",
      "    Loading None.tsv from /home/ashim/Projects/DeepSpeech/data/external/cv-corpus-8.0-2022-01-19/en directory.\n",
      "        \n",
      "    Number of Examples: 864308\n",
      "    \n",
      "    Args:\n",
      "        Sampling Rate: 16000\n",
      "        Output Channels: 1\n",
      "    \n",
      "\n",
      "    CommonVoice Dataset\n",
      "    -------------------\n",
      "    \n",
      "    Loading dev.tsv from /home/ashim/Projects/DeepSpeech/data/external/cv-corpus-8.0-2022-01-19/en directory.\n",
      "        \n",
      "    Number of Examples: 16326\n",
      "    \n",
      "    Args:\n",
      "        Sampling Rate: 16000\n",
      "        Output Channels: 1\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "train_data = CommonVoice(dataset_dir = dataset_dir, subset_path = trimmed_train_path, tokenizer = tokenizer, out_channels = 1)\n",
    "# train_data = CommonVoice(dataset_dir = dataset_dir, subset_name = 'train', tokenizer = tokenizer, out_channels = 1)\n",
    "\n",
    "dev_data = CommonVoice(dataset_dir = dataset_dir, subset_name = 'dev', tokenizer = tokenizer, out_channels = 1)\n",
    "\n",
    "print(train_data)\n",
    "\n",
    "print(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbeb0b01-e6a7-4d25-a816-f00c0c4951db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'encoder_input_size': 80,\n",
    "    'conformer_num_heads': 4,\n",
    "    'conformer_ffn_size': 512,\n",
    "    'conformer_num_layers': 16,\n",
    "    'conformer_conv_kernel_size': 31,\n",
    "    'encoder_rnn_hidden_size': 1024,\n",
    "    'encoder_rnn_num_layers': 1,\n",
    "    'encoder_rnn_bidirectional': True,\n",
    "    'decoder_embedding_size': 300,\n",
    "    'decoder_hidden_size': 1024,\n",
    "    'decoder_num_layers': 1,\n",
    "    'decoder_attn_size': 144,\n",
    "    'dropout': 0.3,\n",
    "    'padding_idx': tokenizer.pad_token_id,\n",
    "    'sos_token_id': tokenizer.bos_token_id,\n",
    "    'eos_token_id': tokenizer.eos_token_id,\n",
    "    'vocab_size': vocab_size,\n",
    "    'batch_first': True,\n",
    "    'device': device,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5065e96-6ddb-464a-bee0-0f9a8620e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(tokenizer, special_tokens = False)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                          batch_size = BATCH_SIZE, \n",
    "                          collate_fn=collator, \n",
    "                          shuffle=True, \n",
    "                          pin_memory = False, \n",
    "                          num_workers = 6, \n",
    "                          worker_init_fn = collator.seed_worker, \n",
    "                          generator = g)\n",
    "\n",
    "fp16 = False\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b39c9e55-bf25-4108-82be-e403984f82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int):\n",
    "        \n",
    "        super(CTCModel, self).__init__()\n",
    "        \n",
    "        self.encoder = torchaudio.models.Conformer(input_dim = 80, num_heads = 16, ffn_dim = 512, num_layers = 12, depthwise_conv_kernel_size=31)\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size = 80, hidden_size = vocab_size)\n",
    "        \n",
    "    def forward(self, x, x_lens) -> torch.Tensor:\n",
    "        \n",
    "        x, x_lens = self.encoder(x, x_lens)\n",
    "        \n",
    "        x, hidden = self.rnn(F.relu(x))\n",
    "        x = F.log_softmax(x, dim = -1)## ctc loss requires log_softmax\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64596bb5-5ab5-4bab-8f4d-b8fdc3b2ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CTCModel(vocab_size=vocab_size).to(device)\n",
    "# model = Model(**model_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d98a8d1d-6e5b-4f9b-8be0-cee5d214c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_summary(encoder, dataloader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef7a38f0-fb52-4c8b-b6c4-471de0d5a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CTC loss should be computed after the encoder outputs the probabilities\n",
    "\n",
    "## Decoding part is usually decoupled from encoding part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "370b2157-d7ea-4a3c-b8e2-b036fa743f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_log_dir = 'logs/'\n",
    "writer = get_writer(base_log_dir=base_log_dir, comment = \"CTC Loss\")\n",
    "# writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10bd7f24-9117-4dbc-aef4-f169695307e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_batch(model: torch.nn.Module, batch: Dict, max_len: int = 50):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    melspecs = batch['melspecs'].to(device).squeeze(0)\n",
    "    melspecs_lengths = batch['melspecs_lengths'].to(device, dtype = torch.int32)\n",
    "    \n",
    "    sentences = batch['sentences'].to(device)\n",
    "    sentence_lengths = batch['sentence_lengths'].to(device=device, dtype = torch.int32)    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sort_indices = torch.argsort(sentence_lengths, descending=True)\n",
    "        \n",
    "        melspecs = melspecs[sort_indices]\n",
    "        melspecs_lengths = melspecs_lengths[sort_indices]\n",
    "        \n",
    "        sentences = sentences[sort_indices]\n",
    "        sentence_lengths = sentence_lengths[sort_indices]\n",
    "        \n",
    "        melspecs = torch.transpose(melspecs, -1, -2) ## Changing to (batch, channel, time, n_mels) from (batch, channel, n_mels, time)\n",
    "\n",
    "        y_preds = model.forward(melspecs, melspecs_lengths)\n",
    "\n",
    "        y_ids = y_preds.argmax(dim = -1)\n",
    "        y_pred = torch.unique_consecutive(y_ids, dim = 1)\n",
    "        \n",
    "        y_pred = tokenizer.batch_decode(y_pred)\n",
    "    \n",
    "    y_true = tokenizer.batch_decode(sentences)\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e33cf11-0a82-4892-a25d-b7f6aea3b68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 3.58 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "try:\n",
    "    samples\n",
    "except NameError as e:\n",
    "    samples = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6175e987-149c-4617-ad32-a13756bbaa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "lr = 0.01 # learning rate\n",
    "\n",
    "MAX_NORM = 0.5\n",
    "\n",
    "num_batches = len(train_loader)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr = lr)\n",
    "# optimizer = torch.optim.RAdam(model.parameters())\n",
    "\n",
    "epoch_end_scheduler = ReduceLROnPlateau(optimizer, mode = 'min', patience = 2)\n",
    "\n",
    "cawr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=2, T_mult=2)\n",
    "\n",
    "batch_end_scheduler = StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "criterion = nn.CTCLoss(blank = tokenizer.vocab['[BLANK]'], \n",
    "                       zero_infinity=True,\n",
    "                       reduction = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "679a0551-4b8c-4a79-b46c-53157937eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch: List[Dict[str, torch.Tensor]], n_iter: int, MAX_NORM: float = 0.5, plot_gradients: bool = True):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    \n",
    "    sentences = batch['sentences'].to(device)\n",
    "    sentence_lengths = batch['sentence_lengths'].to(device, dtype = torch.int32)\n",
    "\n",
    "    melspecs = batch['melspecs'].to(device)\n",
    "    melspecs_lengths = batch['melspecs_lengths'].to(device, dtype = torch.int32)\n",
    "\n",
    "    sort_indices = torch.argsort(sentence_lengths, descending=True)\n",
    "\n",
    "    melspecs = melspecs[sort_indices]\n",
    "    melspecs_lengths = melspecs_lengths[sort_indices]\n",
    "\n",
    "    sentences = sentences[sort_indices]\n",
    "    sentence_lengths = sentence_lengths[sort_indices]\n",
    "\n",
    "    melspecs = torch.transpose(melspecs, -1, -2) ## Changing to (batch, channel, time, n_mels) from (batch, channel, n_mels, time)\n",
    "\n",
    "    y_preds = model.forward(melspecs, melspecs_lengths)\n",
    "    loss = criterion(y_preds.permute(1, 0, 2), sentences, melspecs_lengths, sentence_lengths)\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    ## Plot Gradients every 10 steps\n",
    "    if n_iter % 10 == 0 and plot_gradients == True:\n",
    "\n",
    "        grad_flow_fig = plot_grad_flow_v2(model.named_parameters())\n",
    "    \n",
    "    else:\n",
    "        grad_flow_fig = None\n",
    "    \n",
    "    ## Gradient Clipping for exploding gradients\n",
    "    clip_grad_norm_(model.parameters(), max_norm = MAX_NORM)\n",
    "\n",
    "    ## Step the optimizers\n",
    "    optimizer.step()\n",
    "\n",
    "    ## Step the schedulers\n",
    "    batch_end_scheduler.step()\n",
    "\n",
    "    return loss.detach().cpu().item(), grad_flow_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44cca05e-9bf2-4015-b422-65a37a5ba9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for idx, batch in enumerate(train_loader):\n",
    "#         batch = batch\n",
    "#         sentences = batch['sentences'].to(device)\n",
    "#         sentence_lengths = batch['sentence_lengths'].to(device, dtype = torch.int32)\n",
    "\n",
    "#         melspecs = batch['melspecs'].to(device)\n",
    "#         melspecs_lengths = batch['melspecs_lengths'].to(device, dtype = torch.int32)\n",
    "                \n",
    "#         sort_indices = torch.argsort(sentence_lengths, descending=True)\n",
    "        \n",
    "#         melspecs = melspecs[sort_indices]\n",
    "#         melspecs_lengths = melspecs_lengths[sort_indices]\n",
    "        \n",
    "#         sentences = sentences[sort_indices]\n",
    "#         sentence_lengths = sentence_lengths[sort_indices]\n",
    "        \n",
    "#         melspecs = torch.transpose(melspecs, -1, -2) ## Changing to (batch, channel, time, n_mels) from (batch, channel, n_mels, time)\n",
    "\n",
    "#         y_preds = model.forward(melspecs, melspecs_lengths)\n",
    "#         loss = criterion(y_preds.permute(1, 0, 2), sentences, melspecs_lengths, sentence_lengths)\n",
    "                \n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4acc4f9-c9e2-4234-8fa6-9961c01e082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200\n",
      " 4219/13505 [=>......] - ETA: 42:29 - loss: 10.0525parameter shape is: torch.Size([80, 1, 31]), parameter name is: encoder.conformer_layers.5.conv_module.sequential.2.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff1215cef70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ashim/miniconda3/envs/speech/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/ashim/miniconda3/envs/speech/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1322, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ashim/miniconda3/envs/speech/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/ashim/miniconda3/envs/speech/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/home/ashim/miniconda3/envs/speech/lib/python3.9/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ashim/miniconda3/envs/speech/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_iter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    kbar = pkbar.Kbar(target = num_batches, epoch = epoch, num_epochs=EPOCHS, width = 8, always_stateful=False)\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss, grad_flow_fig = train_step(batch, n_iter, plot_gradients=True)\n",
    "        \n",
    "        ## Write how sample is being predicted\n",
    "        ##predict_one_batch uses no grad\n",
    "        sample_true, sample_pred = predict_one_batch(model, samples)\n",
    "        writer.add_text('sentence predictions', f'true sentence: {sample_true[0]}, predicted sentence: {sample_pred[0]}', global_step = n_iter)\n",
    "        \n",
    "        writer.add_scalar('CE Loss/train', loss, n_iter)\n",
    "        \n",
    "        if grad_flow_fig != None:\n",
    "            \n",
    "            writer.add_figure('Average Gradients/Model', grad_flow_fig, global_step = n_iter, close = True)\n",
    "\n",
    "        kbar.update(idx, values = [(\"loss\", loss)])\n",
    "\n",
    "        n_iter += 1\n",
    "        \n",
    "    \n",
    "    ## At epoch end\n",
    "    \n",
    "    # cawr_scheduler.step() ##cosine annealing with warm restarts\n",
    "    epoch_end_scheduler.step(loss)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20521fa-e306-402f-9f1a-0659a02b9421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8376296fdecafa8d748ac5b3740e0b8e6dc4d67dae6152bd741e7a91aa957642"
  },
  "kernelspec": {
   "display_name": "Speech",
   "language": "python",
   "name": "speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
