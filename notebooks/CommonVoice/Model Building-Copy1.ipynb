{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a05c6a0-a5b7-459d-95cb-810afe750ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau, StepLR\n",
    "from torch.optim import RAdam\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, PackedSequence\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "from src.model.model import *\n",
    "\n",
    "from src.utils.dataset import CommonVoice\n",
    "from src.utils.audio_utils import plot_waveform, play_audio\n",
    "from src.utils.collate import Collator\n",
    "from src.utils.tokenizer import get_tokenizer\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bccc5281-1c6f-490f-b2de-72d7c9187fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f237251-1e2f-402f-a0b0-995c07bf1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.misc import get_summary, get_writer\n",
    "from src.utils.grad_flow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2446aac2-89a2-4747-bb36-41fedc0ef545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pkbar\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6071366-8bd4-490f-800d-64e0439b3dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0 \n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110dab0c-10b1-4665-8726-cc37bcfd6371",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc2acde-600e-4545-9207-76b48e5abe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'data/external/cv-corpus-8.0-2022-01-19/en/'\n",
    "\n",
    "tokenizer_file = 'data/tokenizer/trained_tokenizer.json'\n",
    "\n",
    "trimmed_train_path = 'data/internal/sample_train.tsv'\n",
    "# trimmed_train_path = 'data/internal/train_trimmed.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "277f89b6-6c88-4763-a927-e2f8b8d1b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(tokenizer_file_path=tokenizer_file)\n",
    "\n",
    "blank_token_id = tokenizer.vocab[\"[BLANK]\"]\n",
    "bos_token_id = tokenizer.vocab[\"[BOS]\"]\n",
    "eos_token_id = tokenizer.vocab[\"[EOS]\"]\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4230b8ef-817a-4449-a113-19dc2518c9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    CommonVoice Dataset\n",
      "    -------------------\n",
      "    \n",
      "    Loading None.tsv from /home/ashim/Projects/DeepSpeech/data/external/cv-corpus-8.0-2022-01-19/en directory.\n",
      "        \n",
      "    Number of Examples: 4192\n",
      "    \n",
      "    Args:\n",
      "        Sampling Rate: 16000\n",
      "        Output Channels: 1\n",
      "    \n",
      "\n",
      "    CommonVoice Dataset\n",
      "    -------------------\n",
      "    \n",
      "    Loading dev.tsv from /home/ashim/Projects/DeepSpeech/data/external/cv-corpus-8.0-2022-01-19/en directory.\n",
      "        \n",
      "    Number of Examples: 16326\n",
      "    \n",
      "    Args:\n",
      "        Sampling Rate: 16000\n",
      "        Output Channels: 1\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "train_data = CommonVoice(dataset_dir = dataset_dir, subset_path = trimmed_train_path, tokenizer = tokenizer, out_channels = 1)\n",
    "# train_data = CommonVoice(dataset_dir = dataset_dir, subset_name = 'train', tokenizer = tokenizer, out_channels = 1)\n",
    "\n",
    "dev_data = CommonVoice(dataset_dir = dataset_dir, subset_name = 'dev', tokenizer = tokenizer, out_channels = 1)\n",
    "\n",
    "print(train_data)\n",
    "\n",
    "print(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbeb0b01-e6a7-4d25-a816-f00c0c4951db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'encoder_input_size': 80,\n",
    "    'conformer_num_heads': 4,\n",
    "    'conformer_ffn_size': 512,\n",
    "    'conformer_num_layers': 16,\n",
    "    'conformer_conv_kernel_size': 31,\n",
    "    'encoder_rnn_hidden_size': 1024,\n",
    "    'encoder_rnn_num_layers': 1,\n",
    "    'encoder_rnn_bidirectional': True,\n",
    "    'decoder_embedding_size': 300,\n",
    "    'decoder_hidden_size': 1024,\n",
    "    'decoder_num_layers': 1,\n",
    "    'decoder_attn_size': 144,\n",
    "    'dropout': 0.3,\n",
    "    'padding_idx': tokenizer.pad_token_id,\n",
    "    'sos_token_id': tokenizer.bos_token_id,\n",
    "    'eos_token_id': tokenizer.eos_token_id,\n",
    "    'vocab_size': vocab_size,\n",
    "    'batch_first': True,\n",
    "    'device': device,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5065e96-6ddb-464a-bee0-0f9a8620e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(tokenizer)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                          batch_size = BATCH_SIZE, \n",
    "                          collate_fn=collator, \n",
    "                          shuffle=True, \n",
    "                          pin_memory = False, \n",
    "                          num_workers = 12, \n",
    "                          worker_init_fn = collator.seed_worker, \n",
    "                          generator = g)\n",
    "\n",
    "fp16 = False\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64596bb5-5ab5-4bab-8f4d-b8fdc3b2ec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashim/miniconda3/envs/speech/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = Model(**model_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d98a8d1d-6e5b-4f9b-8be0-cee5d214c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_summary(encoder, dataloader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef7a38f0-fb52-4c8b-b6c4-471de0d5a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CTC loss should be computed after the encoder outputs the probabilities\n",
    "\n",
    "## Decoding part is usually decoupled from encoding part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "370b2157-d7ea-4a3c-b8e2-b036fa743f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_log_dir = 'logs/'\n",
    "writer = get_writer(base_log_dir=base_log_dir)\n",
    "# writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10bd7f24-9117-4dbc-aef4-f169695307e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_batch(model: torch.nn.Module, batch: Dict, max_len: int = 50):\n",
    "    model.eval()\n",
    "    \n",
    "    melspecs = batch['melspecs'].to(device).squeeze(0)\n",
    "    melspecs_lengths = batch['melspecs_lengths'].to(device, dtype = torch.int32)\n",
    "    \n",
    "    sentences = batch['sentences'].to(device)\n",
    "    sentence_lengths = torch.LongTensor(batch['sentence_lengths']).to(device=device)    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ##Change from [batch, feats, seq_len] to [batch, seq_len,featrs]\n",
    "        predicted_tensor = model.forward(melspecs.permute(0,2,1), melspecs_lengths, sentences, sentence_lengths)\n",
    "        y_ids = predicted_tensor.argmax(dim = -1)\n",
    "        y_pred = torch.unique_consecutive(y_ids, dim = 1)\n",
    "        \n",
    "        y_pred = tokenizer.batch_decode(y_pred)\n",
    "    \n",
    "    y_true = tokenizer.batch_decode(sentences)\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a1bacc9-8143-4d16-a5f9-ca7614c44579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch: List[Dict[str, torch.Tensor]], n_iter: int, MAX_NORM: float = 0.5, plot_gradients: bool = True):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    \n",
    "    sentences = batch['sentences'].to(device)\n",
    "    sentence_lengths = batch['sentence_lengths'].to(device, dtype = torch.int32)\n",
    "\n",
    "    melspecs = batch['melspecs'].to(device)\n",
    "    melspecs_lengths = batch['melspecs_lengths'].to(device, dtype = torch.int32)\n",
    "\n",
    "    melspecs = torch.transpose(melspecs, -1, -2) ## Changing to (batch, channel, time, n_mels) from (batch, channel, n_mels, time)\n",
    "\n",
    "    predicted_tensor = model.forward(melspecs, melspecs_lengths, sentences, sentence_lengths)\n",
    "    \n",
    "    y_pred = predicted_tensor.reshape(-1, vocab_size)\n",
    "    y_true = sentences[:, 1:].reshape(-1)  ##Skip the <sos> token from the targets \n",
    "        \n",
    "    loss = criterion( input = y_pred, target = y_true )       \n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    ## Plot Gradients every 10 steps\n",
    "    if n_iter % 10 == 0 and plot_gradients == True:\n",
    "\n",
    "        grad_flow_fig = plot_grad_flow_v2(model.named_parameters())\n",
    "    \n",
    "    else:\n",
    "        grad_flow_fig = None\n",
    "    \n",
    "    ## Gradient Clipping for exploding gradients\n",
    "    clip_grad_norm_(model.parameters(), max_norm = MAX_NORM)\n",
    "\n",
    "    ## Step the optimizers\n",
    "    optimizer.step()\n",
    "\n",
    "    ## Step the schedulers\n",
    "    # batch_end_scheduler.step()\n",
    "\n",
    "    return loss.detach().cpu().item(), grad_flow_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e33cf11-0a82-4892-a25d-b7f6aea3b68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.34 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "try:\n",
    "    samples\n",
    "except NameError as e:\n",
    "    samples = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6175e987-149c-4617-ad32-a13756bbaa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "lr = 0.01 # learning rate\n",
    "\n",
    "MAX_NORM = 0.5\n",
    "\n",
    "num_batches = len(train_loader)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr = lr)\n",
    "# optim = torch.optim.RAdam(model.parameters())\n",
    "\n",
    "epoch_end_scheduler = ReduceLROnPlateau(optimizer, mode = 'min', patience = 2)\n",
    "\n",
    "cawr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=2, T_mult=2)\n",
    "\n",
    "# batch_end_scheduler = StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "criterion = nn.NLLLoss(ignore_index=tokenizer.pad_token_id)\n",
    "# criterion = nn.CTCLoss(blank = tokenizer.vocab['[BLANK]'], \n",
    "#                      zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44cca05e-9bf2-4015-b422-65a37a5ba9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for idx, batch in enumerate(train_loader):\n",
    "#         batch = batch\n",
    "#         sentences = batch['sentences'].to(device)\n",
    "#         sentence_lengths = batch['sentence_lengths'].to(device, dtype = torch.int32)\n",
    "\n",
    "#         melspecs = batch['melspecs'].to(device)\n",
    "#         melspecs_lengths = batch['melspecs_lengths'].to(device, dtype = torch.int32)\n",
    "\n",
    "#         melspecs = torch.transpose(melspecs, -1, -2) ## Changing to (batch, channel, time, n_mels) from (batch, channel, n_mels, time)\n",
    "\n",
    "#         predicted_tensor = model.forward(melspecs, melspecs_lengths, sentences, sentence_lengths)\n",
    "\n",
    "#         y_pred = predicted_tensor.reshape(-1, vocab_size)\n",
    "#         y_true = sentences[:, 1:].reshape(-1)  ##Skip the <sos> token from the targets\n",
    "        \n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4acc4f9-c9e2-4234-8fa6-9961c01e082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 5.6048 - PPL: 286.8838\n",
      "\n",
      "Epoch: 2/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 5.3079 - PPL: 202.0612\n",
      "\n",
      "Epoch: 3/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 5.1951 - PPL: 180.5877\n",
      "\n",
      "Epoch: 4/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 5.0700 - PPL: 159.7477\n",
      "\n",
      "Epoch: 5/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.9949 - PPL: 148.3364\n",
      "\n",
      "Epoch: 6/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 5.0093 - PPL: 150.7114\n",
      "\n",
      "Epoch: 7/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 5.1511 - PPL: 196.3023\n",
      "\n",
      "Epoch: 8/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.9741 - PPL: 145.2393\n",
      "\n",
      "Epoch: 9/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.9506 - PPL: 142.3044\n",
      "\n",
      "Epoch: 10/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.9231 - PPL: 138.4493\n",
      "\n",
      "Epoch: 11/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 5.0274 - PPL: 157.8340\n",
      "\n",
      "Epoch: 12/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 5.2253 - PPL: 4989.3830\n",
      "\n",
      "Epoch: 13/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 5.0398 - PPL: 156.1736\n",
      "\n",
      "Epoch: 14/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.8535 - PPL: 129.8566\n",
      "\n",
      "Epoch: 15/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.7343 - PPL: 115.0507\n",
      "\n",
      "Epoch: 16/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6645 - PPL: 107.0199\n",
      "\n",
      "Epoch: 17/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6426 - PPL: 105.8756\n",
      "\n",
      "Epoch: 18/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6085 - PPL: 101.6619\n",
      "\n",
      "Epoch: 19/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5957 - PPL: 100.1990\n",
      "\n",
      "Epoch: 20/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5668 - PPL: 97.2642\n",
      "\n",
      "Epoch: 21/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5678 - PPL: 97.9628\n",
      "\n",
      "Epoch: 22/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5957 - PPL: 100.0327\n",
      "\n",
      "Epoch: 23/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5766 - PPL: 98.4572\n",
      "\n",
      "Epoch: 24/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6046 - PPL: 101.3173\n",
      "\n",
      "Epoch: 25/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5635 - PPL: 96.7584\n",
      "\n",
      "Epoch: 26/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5828 - PPL: 99.5641 \n",
      "\n",
      "Epoch: 27/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6268 - PPL: 103.2594\n",
      "\n",
      "Epoch: 28/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6054 - PPL: 101.2814\n",
      "\n",
      "Epoch: 29/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6188 - PPL: 102.5229\n",
      "\n",
      "Epoch: 30/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6234 - PPL: 103.3656\n",
      "\n",
      "Epoch: 31/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5995 - PPL: 100.8962\n",
      "\n",
      "Epoch: 32/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5140 - PPL: 92.4625\n",
      "\n",
      "Epoch: 33/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5855 - PPL: 99.4108\n",
      "\n",
      "Epoch: 34/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5824 - PPL: 98.8365\n",
      "\n",
      "Epoch: 35/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5547 - PPL: 96.2788\n",
      "\n",
      "Epoch: 36/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5667 - PPL: 97.1242\n",
      "\n",
      "Epoch: 37/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5066 - PPL: 91.9411\n",
      "\n",
      "Epoch: 38/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5728 - PPL: 98.0308\n",
      "\n",
      "Epoch: 39/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5552 - PPL: 96.7515\n",
      "\n",
      "Epoch: 40/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6015 - PPL: 101.1131\n",
      "\n",
      "Epoch: 41/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6299 - PPL: 103.9121\n",
      "\n",
      "Epoch: 42/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5420 - PPL: 95.2545\n",
      "\n",
      "Epoch: 43/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5893 - PPL: 99.8252 \n",
      "\n",
      "Epoch: 44/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5711 - PPL: 97.6661\n",
      "\n",
      "Epoch: 45/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5700 - PPL: 97.7706\n",
      "\n",
      "Epoch: 46/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6024 - PPL: 101.0726\n",
      "\n",
      "Epoch: 47/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6112 - PPL: 102.0095\n",
      "\n",
      "Epoch: 48/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6235 - PPL: 102.6777\n",
      "\n",
      "Epoch: 49/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5989 - PPL: 100.7781\n",
      "\n",
      "Epoch: 50/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5668 - PPL: 97.2824\n",
      "\n",
      "Epoch: 51/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5637 - PPL: 97.4851\n",
      "\n",
      "Epoch: 52/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5752 - PPL: 98.6819\n",
      "\n",
      "Epoch: 53/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6169 - PPL: 102.0007\n",
      "\n",
      "Epoch: 54/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6154 - PPL: 102.8720\n",
      "\n",
      "Epoch: 55/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5734 - PPL: 98.2172\n",
      "\n",
      "Epoch: 56/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5682 - PPL: 98.0441\n",
      "\n",
      "Epoch: 57/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5124 - PPL: 92.3122\n",
      "\n",
      "Epoch: 58/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5821 - PPL: 99.0645\n",
      "\n",
      "Epoch: 59/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5575 - PPL: 96.5223\n",
      "\n",
      "Epoch: 60/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5564 - PPL: 96.7367\n",
      "\n",
      "Epoch: 61/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5547 - PPL: 96.0079\n",
      "\n",
      "Epoch: 62/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6099 - PPL: 102.1761\n",
      "\n",
      "Epoch: 63/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5457 - PPL: 96.3023\n",
      "\n",
      "Epoch: 64/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5584 - PPL: 96.4553\n",
      "\n",
      "Epoch: 65/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5559 - PPL: 96.2623\n",
      "\n",
      "Epoch: 66/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5685 - PPL: 97.5799\n",
      "\n",
      "Epoch: 67/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5548 - PPL: 96.7228\n",
      "\n",
      "Epoch: 68/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5429 - PPL: 95.1834\n",
      "\n",
      "Epoch: 69/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6134 - PPL: 102.1237\n",
      "\n",
      "Epoch: 70/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5800 - PPL: 98.8179\n",
      "\n",
      "Epoch: 71/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5923 - PPL: 99.9588\n",
      "\n",
      "Epoch: 72/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6059 - PPL: 101.4825\n",
      "\n",
      "Epoch: 73/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5669 - PPL: 97.9948\n",
      "\n",
      "Epoch: 74/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5154 - PPL: 92.6553\n",
      "\n",
      "Epoch: 75/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6293 - PPL: 103.5139\n",
      "\n",
      "Epoch: 76/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5945 - PPL: 100.8008\n",
      "\n",
      "Epoch: 77/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6198 - PPL: 102.7174\n",
      "\n",
      "Epoch: 78/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6046 - PPL: 101.3403\n",
      "\n",
      "Epoch: 79/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5697 - PPL: 97.7203\n",
      "\n",
      "Epoch: 80/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5787 - PPL: 98.9436\n",
      "\n",
      "Epoch: 81/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5510 - PPL: 96.1307\n",
      "\n",
      "Epoch: 82/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5778 - PPL: 98.5348\n",
      "\n",
      "Epoch: 83/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5837 - PPL: 98.6861\n",
      "\n",
      "Epoch: 84/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5883 - PPL: 99.4645\n",
      "\n",
      "Epoch: 85/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5359 - PPL: 94.7955\n",
      "\n",
      "Epoch: 86/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5853 - PPL: 99.4269\n",
      "\n",
      "Epoch: 87/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5488 - PPL: 95.2083\n",
      "\n",
      "Epoch: 88/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5572 - PPL: 96.3907\n",
      "\n",
      "Epoch: 89/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5448 - PPL: 95.2174\n",
      "\n",
      "Epoch: 90/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5874 - PPL: 100.3637\n",
      "\n",
      "Epoch: 91/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5772 - PPL: 98.3673\n",
      "\n",
      "Epoch: 92/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5638 - PPL: 97.6299\n",
      "\n",
      "Epoch: 93/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5473 - PPL: 96.0499\n",
      "\n",
      "Epoch: 94/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5837 - PPL: 98.7446\n",
      "\n",
      "Epoch: 95/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5703 - PPL: 97.8162\n",
      "\n",
      "Epoch: 96/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5303 - PPL: 94.3106\n",
      "\n",
      "Epoch: 97/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6134 - PPL: 102.2280\n",
      "\n",
      "Epoch: 98/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5651 - PPL: 97.3088\n",
      "\n",
      "Epoch: 99/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5745 - PPL: 98.4305\n",
      "\n",
      "Epoch: 100/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5831 - PPL: 99.1688\n",
      "\n",
      "Epoch: 101/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5524 - PPL: 96.0895\n",
      "\n",
      "Epoch: 102/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5771 - PPL: 98.3826\n",
      "\n",
      "Epoch: 103/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6291 - PPL: 103.5051\n",
      "\n",
      "Epoch: 104/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5549 - PPL: 96.6338\n",
      "\n",
      "Epoch: 105/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5475 - PPL: 96.2604\n",
      "\n",
      "Epoch: 106/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5326 - PPL: 94.6684\n",
      "\n",
      "Epoch: 107/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6268 - PPL: 103.6813\n",
      "\n",
      "Epoch: 108/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5405 - PPL: 94.8534\n",
      "\n",
      "Epoch: 109/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5934 - PPL: 99.9786\n",
      "\n",
      "Epoch: 110/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6096 - PPL: 101.3823\n",
      "\n",
      "Epoch: 111/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5546 - PPL: 95.9592\n",
      "\n",
      "Epoch: 112/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5486 - PPL: 95.4592\n",
      "\n",
      "Epoch: 113/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6086 - PPL: 101.7589\n",
      "\n",
      "Epoch: 114/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5937 - PPL: 99.9012 \n",
      "\n",
      "Epoch: 115/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5514 - PPL: 96.0573\n",
      "\n",
      "Epoch: 116/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5435 - PPL: 94.8066\n",
      "\n",
      "Epoch: 117/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5558 - PPL: 96.8705\n",
      "\n",
      "Epoch: 118/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5865 - PPL: 99.4244\n",
      "\n",
      "Epoch: 119/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6090 - PPL: 100.9617\n",
      "\n",
      "Epoch: 120/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5825 - PPL: 99.2872\n",
      "\n",
      "Epoch: 121/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6010 - PPL: 101.0227\n",
      "\n",
      "Epoch: 122/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5313 - PPL: 94.2715\n",
      "\n",
      "Epoch: 123/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6244 - PPL: 103.1088\n",
      "\n",
      "Epoch: 124/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5569 - PPL: 96.8261\n",
      "\n",
      "Epoch: 125/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5912 - PPL: 99.5937\n",
      "\n",
      "Epoch: 126/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5580 - PPL: 96.7280\n",
      "\n",
      "Epoch: 127/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5829 - PPL: 99.1615\n",
      "\n",
      "Epoch: 128/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5543 - PPL: 96.1107\n",
      "\n",
      "Epoch: 129/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6326 - PPL: 103.9094\n",
      "\n",
      "Epoch: 130/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6118 - PPL: 101.7787\n",
      "\n",
      "Epoch: 131/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6322 - PPL: 104.2670\n",
      "\n",
      "Epoch: 132/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5993 - PPL: 101.0911\n",
      "\n",
      "Epoch: 133/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6012 - PPL: 100.6079\n",
      "\n",
      "Epoch: 134/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6061 - PPL: 100.8982\n",
      "\n",
      "Epoch: 135/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6248 - PPL: 102.9765\n",
      "\n",
      "Epoch: 136/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5308 - PPL: 93.8641\n",
      "\n",
      "Epoch: 137/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5750 - PPL: 98.8104\n",
      "\n",
      "Epoch: 138/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5797 - PPL: 98.5728\n",
      "\n",
      "Epoch: 139/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5991 - PPL: 100.2090\n",
      "\n",
      "Epoch: 140/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5126 - PPL: 91.6966\n",
      "\n",
      "Epoch: 141/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5744 - PPL: 98.1472\n",
      "\n",
      "Epoch: 142/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5314 - PPL: 94.0470\n",
      "\n",
      "Epoch: 143/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5756 - PPL: 98.4077\n",
      "\n",
      "Epoch: 144/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5432 - PPL: 95.8022\n",
      "\n",
      "Epoch: 145/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5678 - PPL: 97.3957\n",
      "\n",
      "Epoch: 146/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5949 - PPL: 100.6640\n",
      "\n",
      "Epoch: 147/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5505 - PPL: 96.2176\n",
      "\n",
      "Epoch: 148/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5723 - PPL: 98.6822\n",
      "\n",
      "Epoch: 149/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5587 - PPL: 96.8322\n",
      "\n",
      "Epoch: 150/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5775 - PPL: 98.2728\n",
      "\n",
      "Epoch: 151/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6104 - PPL: 102.3755\n",
      "\n",
      "Epoch: 152/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5576 - PPL: 96.9689\n",
      "\n",
      "Epoch: 153/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5576 - PPL: 97.2097\n",
      "\n",
      "Epoch: 154/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5440 - PPL: 95.5293\n",
      "\n",
      "Epoch: 155/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5301 - PPL: 94.1790\n",
      "\n",
      "Epoch: 156/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5691 - PPL: 97.7936\n",
      "\n",
      "Epoch: 157/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5669 - PPL: 97.3961\n",
      "\n",
      "Epoch: 158/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5086 - PPL: 92.0173\n",
      "\n",
      "Epoch: 159/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5860 - PPL: 98.9694\n",
      "\n",
      "Epoch: 160/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5574 - PPL: 96.6293\n",
      "\n",
      "Epoch: 161/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5287 - PPL: 93.8971\n",
      "\n",
      "Epoch: 162/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5795 - PPL: 98.9430\n",
      "\n",
      "Epoch: 163/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5329 - PPL: 94.5735\n",
      "\n",
      "Epoch: 164/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6753 - PPL: 108.7018\n",
      "\n",
      "Epoch: 165/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6304 - PPL: 103.7182\n",
      "\n",
      "Epoch: 166/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5326 - PPL: 94.1226\n",
      "\n",
      "Epoch: 167/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5547 - PPL: 96.3028\n",
      "\n",
      "Epoch: 168/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5596 - PPL: 96.9037\n",
      "\n",
      "Epoch: 169/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5570 - PPL: 96.3044\n",
      "\n",
      "Epoch: 170/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5396 - PPL: 95.5851\n",
      "\n",
      "Epoch: 171/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6133 - PPL: 102.4999\n",
      "\n",
      "Epoch: 172/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5457 - PPL: 95.2997\n",
      "\n",
      "Epoch: 173/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5335 - PPL: 93.9561\n",
      "\n",
      "Epoch: 174/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5785 - PPL: 98.6443\n",
      "\n",
      "Epoch: 175/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6070 - PPL: 100.7406\n",
      "\n",
      "Epoch: 176/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5791 - PPL: 98.6893\n",
      "\n",
      "Epoch: 177/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6348 - PPL: 104.1825\n",
      "\n",
      "Epoch: 178/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5940 - PPL: 100.0241\n",
      "\n",
      "Epoch: 179/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6015 - PPL: 101.4419\n",
      "\n",
      "Epoch: 180/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5849 - PPL: 99.2495\n",
      "\n",
      "Epoch: 181/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5946 - PPL: 100.0380\n",
      "\n",
      "Epoch: 182/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6023 - PPL: 100.8112\n",
      "\n",
      "Epoch: 183/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6200 - PPL: 103.0631\n",
      "\n",
      "Epoch: 184/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5420 - PPL: 94.7530\n",
      "\n",
      "Epoch: 185/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6061 - PPL: 101.3730\n",
      "\n",
      "Epoch: 186/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6053 - PPL: 101.1626\n",
      "\n",
      "Epoch: 187/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5916 - PPL: 99.7469 \n",
      "\n",
      "Epoch: 188/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5594 - PPL: 96.7856\n",
      "\n",
      "Epoch: 189/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6022 - PPL: 101.3521\n",
      "\n",
      "Epoch: 190/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5843 - PPL: 99.8626\n",
      "\n",
      "Epoch: 191/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5753 - PPL: 98.6833\n",
      "\n",
      "Epoch: 192/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5493 - PPL: 95.6503\n",
      "\n",
      "Epoch: 193/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5849 - PPL: 100.3030\n",
      "\n",
      "Epoch: 194/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6032 - PPL: 101.0180\n",
      "\n",
      "Epoch: 195/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6514 - PPL: 106.0723\n",
      "\n",
      "Epoch: 196/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5771 - PPL: 97.8994\n",
      "\n",
      "Epoch: 197/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5704 - PPL: 98.2270\n",
      "\n",
      "Epoch: 198/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5962 - PPL: 100.5258\n",
      "\n",
      "Epoch: 199/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.6031 - PPL: 101.1791\n",
      "\n",
      "Epoch: 200/200\n",
      "32/33 [======>.] - ETA: 1s - loss: 4.5858 - PPL: 98.9909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_iter = 0\n",
    "\n",
    "y_true, _ = predict_one_batch(model, samples)\n",
    "writer.add_text('y_true sentence', y_true[0], global_step = n_iter)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    kbar = pkbar.Kbar(target = num_batches, epoch = epoch, num_epochs=EPOCHS, width = 8, always_stateful=False)\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss, grad_flow_fig = train_step(batch, n_iter, plot_gradients=True)\n",
    "        \n",
    "        ## Write how sample is being predicted\n",
    "        ##predict_one_batch uses no grad\n",
    "        sample_true, sample_pred = predict_one_batch(model, samples)\n",
    "        sample_true_0 = sample_true[0]\n",
    "        sample_true_0 = sample_true_0[sample_true_0 != tokenizer.pad_token_id]\n",
    "        \n",
    "        writer.add_text('sentence predictions', f'true sentence: {sample_true_0}, predicted sentence: {sample_pred[0]}', global_step = n_iter)\n",
    "        \n",
    "        writer.add_scalar('CE Loss/train', loss, n_iter)\n",
    "        \n",
    "        if grad_flow_fig != None:\n",
    "            \n",
    "            writer.add_figure('Average Gradients/Model', grad_flow_fig, global_step = n_iter, close = True)\n",
    "\n",
    "        kbar.update(idx, values = [(\"loss\", loss), (\"PPL\", math.exp(loss))])\n",
    "\n",
    "        n_iter += 1\n",
    "    \n",
    "    ## At epoch end\n",
    "    \n",
    "    # cawr_scheduler.step() ##cosine annealing with warm restarts\n",
    "    epoch_end_scheduler.step(loss)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20521fa-e306-402f-9f1a-0659a02b9421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8376296fdecafa8d748ac5b3740e0b8e6dc4d67dae6152bd741e7a91aa957642"
  },
  "kernelspec": {
   "display_name": "Speech",
   "language": "python",
   "name": "speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
