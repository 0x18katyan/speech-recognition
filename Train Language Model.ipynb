{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c75a9971-d04b-4eec-b785-6ab4568923e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from utils.tokenizer import get_tokenizer\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff432ef5-32fa-46a5-8d8d-20cd2849363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e15e38-aeb5-4850-bb15-a424c61993ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get the checkpoint with the highest number of steps\n",
    "checkpoint_path = 'data/model/custom-lm/'\n",
    "model_checkpoints = glob.glob(os.path.join(checkpoint_path, 'checkpoint-*'))\n",
    "latest_checkpoint = 'checkpoint-' + max([checkpoint.split('/')[-1].split('-')[-1] for checkpoint in model_checkpoints])\n",
    "\n",
    "latest_checkpoint_path = os.path.join(checkpoint_path, latest_checkpoint)\n",
    "\n",
    "if os.path.exists(latest_checkpoint_path) == False:\n",
    "    raise ValueError(\"Cannot find the latest checkpoint path. Bug in code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a13d05c5-6d50-43db-8126-a22a7c7fa8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('data/tokenizer/trained_tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4796f313-a2b0-4185-913a-38e38f77de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig(vocab_size = 1000, \n",
    "                       max_position_embeddings = 514, \n",
    "                       num_attention_heads = 12, \n",
    "                       num_hidden_layers = 6, \n",
    "                       type_vocab_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb315d6d-5d98-4df9-bde1-2c7cb47e21be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bc07015-f001-4f79-9233-b6409d7d7c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashim/miniconda3/envs/speech/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = LineByLineTextDataset(tokenizer = tokenizer, \n",
    "                                file_path = \"data/internal/train_lm.txt\", \n",
    "                                block_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc91be77-bcf8-4872-b1b5-c4ac1fdd9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm = True, mlm_probability = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a00192-ec8b-4f01-897e-491fe05f36bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = 'data/model/custom-lm', \n",
    "                                  overwrite_output_dir = False, \n",
    "                                  num_train_epochs = 100, \n",
    "                                  per_device_train_batch_size = 512, \n",
    "                                  save_steps = 10_000, \n",
    "                                  save_total_limit = 2, \n",
    "                                  prediction_loss_only=True, \n",
    "                                  dataloader_num_workers = 2,\n",
    "                                  bf16 = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a800cd0-f4d9-420a-8a66-4dd2699310c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model = model, \n",
    "                  args = training_args, \n",
    "                  data_collator = data_collator, \n",
    "                  train_dataset = dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c8e41-6e38-490a-b6f0-fb7d91db40ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from data/model/custom-lm/checkpoint-70000).\n",
      "***** Running training *****\n",
      "  Num examples = 1530382\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 512\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 299000\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 23\n",
      "  Continuing training from global step 70000\n",
      "  Will skip the first 23 epochs then the first 1230 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755f5af690c1476e9a09c258632031c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(latest_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f6a03-4124-4b5b-8bda-dd2cb0795036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Speech",
   "language": "python",
   "name": "speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
