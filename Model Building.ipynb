{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c9692be-7a3d-40e2-bd87-cdbccbf1c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a05c6a0-a5b7-459d-95cb-810afe750ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.functional as TAF\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.data import load_sp_model, generate_sp_model\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# from data_utils import CommonVoice\n",
    "from utils.audio_utils import plot_waveform, play_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "af8815a9-c5bc-4d3d-815b-751972a47ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98367d89-5617-48c8-989d-e4b0a6e77006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78560c6e-e412-45ff-8282-58faf5284490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2446aac2-89a2-4747-bb36-41fedc0ef545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e3f8f35-221a-44d3-8640-caa0fb67ba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "0.11.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fcc9f4a-d2df-4976-a433-b01d3704ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/external/cv-corpus-8.0-2022-01-19/en/train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e93eba5-fb68-47a3-890e-40b4968e775f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accents</th>\n",
       "      <th>locale</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4225d310188cfd17c5901d29ca5d9685eac936287ed275...</td>\n",
       "      <td>common_voice_en_28449980.mp3</td>\n",
       "      <td>Thereafter the class was highly respected.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4225d310188cfd17c5901d29ca5d9685eac936287ed275...</td>\n",
       "      <td>common_voice_en_28449981.mp3</td>\n",
       "      <td>Banaras Hindu University is a Central Universi...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4225d310188cfd17c5901d29ca5d9685eac936287ed275...</td>\n",
       "      <td>common_voice_en_28449984.mp3</td>\n",
       "      <td>On display are home furnishings, pioneer tools...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4225d310188cfd17c5901d29ca5d9685eac936287ed275...</td>\n",
       "      <td>common_voice_en_28449986.mp3</td>\n",
       "      <td>Eleva and Strum each house an elementary school.</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>425089f4d7e24cdf6861d0130323ec2e41bfc19e35bce5...</td>\n",
       "      <td>common_voice_en_20293200.mp3</td>\n",
       "      <td>The eastern portion of the county lies within ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           client_id  \\\n",
       "0  4225d310188cfd17c5901d29ca5d9685eac936287ed275...   \n",
       "1  4225d310188cfd17c5901d29ca5d9685eac936287ed275...   \n",
       "2  4225d310188cfd17c5901d29ca5d9685eac936287ed275...   \n",
       "3  4225d310188cfd17c5901d29ca5d9685eac936287ed275...   \n",
       "4  425089f4d7e24cdf6861d0130323ec2e41bfc19e35bce5...   \n",
       "\n",
       "                           path  \\\n",
       "0  common_voice_en_28449980.mp3   \n",
       "1  common_voice_en_28449981.mp3   \n",
       "2  common_voice_en_28449984.mp3   \n",
       "3  common_voice_en_28449986.mp3   \n",
       "4  common_voice_en_20293200.mp3   \n",
       "\n",
       "                                            sentence  up_votes  down_votes  \\\n",
       "0         Thereafter the class was highly respected.         2           0   \n",
       "1  Banaras Hindu University is a Central Universi...         2           0   \n",
       "2  On display are home furnishings, pioneer tools...         2           0   \n",
       "3   Eleva and Strum each house an elementary school.         2           1   \n",
       "4  The eastern portion of the county lies within ...         2           0   \n",
       "\n",
       "   age gender accents locale  segment  \n",
       "0  NaN    NaN     NaN     en      NaN  \n",
       "1  NaN    NaN     NaN     en      NaN  \n",
       "2  NaN    NaN     NaN     en      NaN  \n",
       "3  NaN    NaN     NaN     en      NaN  \n",
       "4  NaN    NaN     NaN     en      NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e7d8f8-7c87-47d4-8c58-ff44d6f59798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>864448.000000</td>\n",
       "      <td>864448.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.140156</td>\n",
       "      <td>0.173734</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.519669</td>\n",
       "      <td>0.408876</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>94.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            up_votes     down_votes  segment\n",
       "count  864448.000000  864448.000000      0.0\n",
       "mean        2.140156       0.173734      NaN\n",
       "std         0.519669       0.408876      NaN\n",
       "min         2.000000       0.000000      NaN\n",
       "25%         2.000000       0.000000      NaN\n",
       "50%         2.000000       0.000000      NaN\n",
       "75%         2.000000       0.000000      NaN\n",
       "max        94.000000       6.000000      NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2c928-5c1c-4cf3-8f49-2898fc098662",
   "metadata": {},
   "source": [
    "## Findings\n",
    "\n",
    "1. Train, Test, Dev are the main datasets to be used for train, test, dev.\n",
    "2. There are genders, age and accent columns but they are sparse.\n",
    "3. Segment column is always empty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18836feb-1541-48fc-a586-dd21e4ea34aa",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. Will need a Dataset Loading functionality. Can use path for that.\n",
    "2. Will need a Language Tokenizer and Encoder.\n",
    "3. Will need a speech encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3fc2acde-600e-4545-9207-76b48e5abe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetPATH = 'data/external/cv-corpus-8.0-2022-01-19/en/'\n",
    "clipsPATH = os.path.join(datasetPATH, 'clips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d4d0bd2d-50c6-498a-9e4d-7e0fb789422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonVoice(Dataset):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "        \n",
    "    def __init__(self, dataset_path: str, split_type: str = 'train', out_channels: int = 2, out_sampling_rate: int = 32000, tokenizer = None):\n",
    "        \n",
    " \n",
    "        \"\"\"\n",
    "        Iterate over a split of CommonVoice dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset_path: str\n",
    "            The path where train.tsv, test.tsv and dev.tsv are located.\n",
    "        \n",
    "        split_type: str [train, test, dev]\n",
    "            Loads one of train.tsv, test.tsv or dev.tsv\n",
    "        \n",
    "        out_channels: int \n",
    "            Number of output channels for audio. \n",
    "            Mono = 1, Stereo = 2.\n",
    "        \n",
    "        out_sampling_rate: int\n",
    "            sampling_rate used for standardizing.\n",
    "        \n",
    "        tokenizer: transformers.PreTrainedTokenizerFast\n",
    "            tokenizer: tokenizer from huggingface used for tokenizing.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(CommonVoice).__init__()\n",
    "                \n",
    "        self.split_type = split_type\n",
    "        self.dataset_path = dataset_path\n",
    "        \n",
    "        ##Check that out_sampling_rate is an integer. Will probably need to specify how to convert Khz to Hz.\n",
    "        assert isinstance(out_sampling_rate, int)\n",
    "        self.out_sampling_rate = out_sampling_rate\n",
    "        \n",
    "        ##Check how many output channels are needed.\n",
    "        if out_channels in [1, 2]:\n",
    "            self.out_channels = out_channels\n",
    "        else:\n",
    "            raise ValueError(\"Only Mono (out_channels = 1) and Stereo (out_channels = 2) Supported.\")\n",
    "        \n",
    "        ##Check that dataset exists in the path specified and add clips path.\n",
    "        if os.path.exists(dataset_path):\n",
    "            self.clips_path = os.path.join(dataset_path, 'clips')\n",
    "        else:\n",
    "            raise ValueError(f\"{dataset_path} doesn't exist, please provide a valid path to the dataset.\")\n",
    "        \n",
    "        ##Check that split_type is one of train, test, dev.\n",
    "        if split_type in ['train', 'test' , 'dev']:\n",
    "            fullpath = os.path.join(dataset_path, split_type + '.tsv')\n",
    "        else:\n",
    "            raise ValueError(\"split_type must be one of train, test or dev\")\n",
    "        \n",
    "        ## Load the dataframe\n",
    "        self.dataframe = pd.read_csv(fullpath, sep = '\\t')\n",
    "        \n",
    "        ## Check if tokenizer is passed\n",
    "        \n",
    "        if tokenizer == None:\n",
    "            raise ValueError(\"tokenizer cannot be None.\")\n",
    "        else:\n",
    "            self.tokenizer = tokenizer\n",
    "            \n",
    "        ## Initialize Preprocessing\n",
    "        \n",
    "        self.preprocessing = Preprocessing(out_channels= self.out_channels, out_sampling_rate = self.out_sampling_rate, tokenizer = self.tokenizer)\n",
    "        \n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataframe)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx: Union[int, torch.Tensor, np.ndarray]) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = list(idx)\n",
    "            \n",
    "        item = self.dataframe.iloc[idx]\n",
    "        \n",
    "        sentence = item['sentence']\n",
    "        \n",
    "        age = item['age']\n",
    "        gender = item['gender']\n",
    "        accent = item['accents']\n",
    "        \n",
    "        audio_file_path = os.path.join(self.clips_path, item['path'])\n",
    "        \n",
    "        waveform, source_sampling_rate = torchaudio.load(audio_file_path)\n",
    "        waveform, out_sampling_rate = self.preprocessing.preprocess_waveform(waveform, source_sampling_rate)\n",
    "        \n",
    "        melspec = self.preprocessing.extract_features(waveform)\n",
    "        \n",
    "        # item = {'waveform': waveform, 'sentence': sentence, 'age': age, 'gender' : gender, 'accent': accent}\n",
    "        \n",
    "        item = {'waveform': waveform, 'sentence': sentence, 'melspec': melspec}\n",
    "        \n",
    "        return item\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \\\n",
    "    f\"\"\"\n",
    "    CommonVoice Dataset\n",
    "    -------------------\n",
    "    \n",
    "    Loading {self.split_type}.tsv from {os.path.abspath(self.dataset_path)} directory.\n",
    "        \n",
    "    Number of Examples: {self.__len__()}\n",
    "    \n",
    "    Args:\n",
    "        Sampling Rate: {self.out_sampling_rate}\n",
    "        Output Channels: {self.out_channels}\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f6cda202-12c4-4dc8-b0bd-b2960c8d3276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \"\"\"\n",
    "    Preprocessing contains utilities for transforming audio and text.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    from torchaudio import transforms as T\n",
    "    \n",
    "    def __init__(self, out_channels: int = 2 , out_sampling_rate: int = 32000, n_fft : int = 1024, n_mels: int = 128, tokenizer = None):\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.out_sampling_rate = out_sampling_rate\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.n_fft = n_fft\n",
    "        self.n_mels = n_mels\n",
    "    \n",
    "        self.mel_spec_transform = T.MelSpectrogram(sample_rate = self.out_sampling_rate,\n",
    "                                                   n_fft = self.n_fft,\n",
    "                                                  n_mels = self.n_mels)\n",
    "        \n",
    "    def standardize_channels(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Standardizes number of channels in a waveform. \n",
    "        \n",
    "        Args:\n",
    "            waveform: torch.Tensor (channel, timesteps)\n",
    "        \n",
    "        Returns:\n",
    "            waveform: torch.Tensor (channel, timesteps)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        num_channels = waveform.shape[0]\n",
    "        \n",
    "        if num_channels == self.out_channels:\n",
    "            return waveform\n",
    "        \n",
    "        elif num_channels == 1:            \n",
    "            return torch.cat((waveform, waveform))\n",
    "        \n",
    "        elif num_channels > 2:\n",
    "            raise TypeError(f\"Audio with more than 2 channels are not supported. Wanted maximum of 2 channels, found {num_channels} channels.\")\n",
    "        \n",
    "        elif self.out_channels == 1:\n",
    "            return torch.sum(waveform) / num_channels\n",
    "        \n",
    "        \n",
    "    def standardize_sampling_rate(self, waveform: torch.Tensor, sampling_rate: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "        Standardize Sampling Rate\n",
    "        \n",
    "        Args:\n",
    "            waveform: torch.Tensor\n",
    "            sampling_rate: torch.Tensor\n",
    "            \n",
    "        Returns:\n",
    "            waveform: torch.Tensor\n",
    "            sampling_rate: torch.Tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        ##If there are more than 1 channels\n",
    "        if waveform.shape[0] > 1:\n",
    "        \n",
    "            resampled = []\n",
    "\n",
    "            for i in range(waveform.shape[0]):\n",
    "                resampler = T.Resample(sampling_rate, self.out_sampling_rate)\n",
    "                resampled_channel = resampler(waveform[i, :])\n",
    "                resampled.append(resampled_channel)\n",
    "        \n",
    "            resampled = torch.stack(resampled)\n",
    "            return resampled, self.out_sampling_rate\n",
    "        \n",
    "        ##If there is only 1 channel\n",
    "        else:\n",
    "            return T.Resample(sampling_rate, self.out_sampling_rate)(waveform[0, :]), self.out_sampling_rate\n",
    "\n",
    "    def extract_features(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Applies Feature Extraction Transforms to a waveform.\n",
    "        \n",
    "        Args: waveform\n",
    "        \n",
    "            waveform: The input for the feature extraction.\n",
    "            \n",
    "        Returns: melspec\n",
    "        \n",
    "            melspec: The Mel Spectrogram constructed from the input waveform of shape (channel, n_mels, time).\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.mel_spec_transform(waveform)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def preprocess_waveform(self, waveform: torch.Tensor, sampling_rate: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "        \n",
    "        Preprocess Waveforms. Standardizes channels and sampling rate.\n",
    "        \n",
    "        Args: waveform, sampling_rate\n",
    "        \n",
    "            waveform: torch.Tensor\n",
    "            \n",
    "            sampling_rate: torch.Tensor\n",
    "            \n",
    "        Returns:\n",
    "\n",
    "            waveform: torch.Tensor\n",
    "            \n",
    "            sampling_rate: torch.Tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        waveform = self.standardize_channels(waveform)\n",
    "        \n",
    "        waveform, out_sampling_rate = self.standardize_sampling_rate(waveform, sampling_rate)\n",
    "        \n",
    "        return waveform, out_sampling_rate\n",
    "    \n",
    "    def tokenize_sentence(self, sentence: str):\n",
    "        \"\"\"\n",
    "        Tokenizes a given sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentence: string to be tokenized\n",
    "        \n",
    "        Returns:\n",
    "            tokens: list of tokens numericalized\n",
    "        \"\"\"\n",
    "\n",
    "        encoded = self.tokenizer.encode(sentence)\n",
    "        \n",
    "        return encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8462f0d-1081-45e3-9be8-99381ad356ef",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "### Collate Function\n",
    "\n",
    "1. Pad to highest size\n",
    "\n",
    "### Transforms\n",
    "1. Tokenize and Apply Emedding to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5f579443-e34a-42f7-a4e0-e49d1eed5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    \"\"\"\n",
    "    \n",
    "    Utility Class for Collation of Batch. Intended to be used as part of the PyTorch DataLoader.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer = None):\n",
    "        \"\"\"\n",
    "        Initializes the Collator.\n",
    "        \n",
    "        Args: tokenizer\n",
    "        \n",
    "            tokenizer: Should be a HuggingFace PreTrained Tokenizer.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def pad(self, tensor: torch.Tensor, target_length: int) -> torch.Tensor:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Pads tensor according to the given length.\n",
    "        \n",
    "        Args:\n",
    "            tensor: torch.Tensor\n",
    "                tensor to be padded\n",
    "        \n",
    "            out_length: int\n",
    "                target length of tensor\n",
    "        \n",
    "        Returns:\n",
    "            tensor: torch.Tensor\n",
    "                padded tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        if not isinstance(target_length, int):\n",
    "            raise ValueError(f\"target_length must be an integer. Wanted {int}, have {type(target_length)}\")\n",
    "        \n",
    "        length = target_length - tensor.shape[-1]\n",
    "\n",
    "        if length <= 0:\n",
    "            return tensor\n",
    "        \n",
    "        else:\n",
    "            return F.pad(tensor, (0, length), \"constant\", 0)\n",
    "    \n",
    "    def __call__(self, batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Transforms lists of inputs in the batch to Tensors.\n",
    "            \n",
    "            1. Applies Tokenization to Sentences.\n",
    "            \n",
    "            2. Pads Waveforms and Mel Specs to the maximum lengths in the bacth.\n",
    "        \n",
    "        Args: batch\n",
    "        \n",
    "            Batch containing the list of dicts from the CommonVoice Dataset.\n",
    "            \n",
    "        Returns: batch\n",
    "        \n",
    "            Dict of Tensors.\n",
    "            \n",
    "            dict = {\"waveforms\": *padded waveforms*, // (batch, channel, time, amplitude) \n",
    "                \"sentences\": *padded and tokenized sentences*, // (batch, tokens) \n",
    "                \"mel_specs\": *padded mel spectrograms: *} // (batch, channel, n_mels, timeframes)\n",
    "        \"\"\"\n",
    "    \n",
    "        waveforms = [sample['waveform'] for sample in batch]\n",
    "        sentences = [sample['sentence'] for sample in batch]\n",
    "        melspecs = [sample['melspec'] for sample in batch]\n",
    "        \n",
    "        waveform_lengths = torch.Tensor([waveform.shape[-1] for waveform in waveforms])\n",
    "        \n",
    "        melspecs_lengths = torch.Tensor([melspec.shape[-1] for melspec in melspecs])\n",
    "        \n",
    "        max_len_waveform = int(waveform_lengths.max())\n",
    "        \n",
    "        max_len_melspecs = int(melspecs_lengths.max())\n",
    "\n",
    "        padded_sentences = self.tokenizer(sentences, padding=True, return_tensors = 'pt', return_attention_mask=False)\n",
    "\n",
    "        padded_waveforms = torch.stack([self.pad(waveform, max_len_waveform) for waveform in waveforms])\n",
    "        \n",
    "        padded_mel_specs = torch.stack([self.pad(melspec, max_len_melspecs) for melspec in melspecs])\n",
    "        \n",
    "        return {\"waveforms\": padded_waveforms, \"waveforms_lengths\": waveform_lengths, \"sentences\": padded_sentences['input_ids'], \"melspecs\": padded_mel_specs, \"melspecs_lengths\": melspecs_lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "bce727cc-c1cb-4477-9ac9-327c355fe5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6a9d81a9-6042-486e-9864-71163a57929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: ##Check if tokenizer is defined\n",
    "    tokenizer\n",
    "except NameError as e: ## If tokenizer is not defined then initialize it\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4230b8ef-817a-4449-a113-19dc2518c9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    CommonVoice Dataset\n",
       "    -------------------\n",
       "    \n",
       "    Loading train.tsv from /home/ashim/Projects/DeepSpeech/data/external/cv-corpus-8.0-2022-01-19/en directory.\n",
       "        \n",
       "    Number of Examples: 864448\n",
       "    \n",
       "    Args:\n",
       "        Sampling Rate: 32000\n",
       "        Output Channels: 2\n",
       "    "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = CommonVoice(dataset_path = datasetPATH, split_type = 'train', tokenizer = tokenizer)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "1a39c7d2-6f0a-4c26-a634-509801ff0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim: int = 128, num_heads: int = 4, ffn_dim: int = 128, num_layers: int = 4, depthwise_conv_kernel_size: int = 31, dropout: float = 0.3):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.model = torchaudio.models.Conformer(input_dim = input_dim,\n",
    "                                                 num_heads = num_heads,\n",
    "                                                 ffn_dim = ffn_dim,\n",
    "                                                 num_layers = num_layers,\n",
    "                                                 depthwise_conv_kernel_size = depthwise_conv_kernel_size,\n",
    "                                                 dropout = dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, x_len: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        x, _ = self.model.forward(x, x_len)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "66d7d3a6-3d45-4011-8035-fb116f66e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim: int = 128, hidden_size: int = 256, num_layers: int = 2, bidirectional: bool = False, output_dim: int = None):\n",
    "        \n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        \n",
    "        if output_dim == None:\n",
    "            raise ValueError(\"Please specify the output size of the vocab.\")\n",
    "            \n",
    "        self.model = nn.LSTM(input_size = input_dim, hidden_size = hidden_size, num_layers = num_layers, batch_first = True)\n",
    "        \n",
    "        directions = 2 if bidirectional == True else 1\n",
    "        \n",
    "        self.ffn = nn.Linear(in_features = hidden_size * directions, out_features = output_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        bsz, msl, hdz = x.shape ##batch_size, max sequence length, hidden dimension size\n",
    "        \n",
    "        decoded = []\n",
    "        \n",
    "        for t in range(msl):\n",
    "            \n",
    "            t_sample = x[:, t, :]\n",
    "            \n",
    "            if t == 0: ## Check if h_v variable is initialized\n",
    "                output, (h_v, c_v) = self.model(t_sample)\n",
    "            \n",
    "            else:    \n",
    "                output, (h_v, c_v) = self.model(t_sample, (h_v, c_v))\n",
    "                \n",
    "            word = F.softmax(self.ffn(output), dim = -1).argmax(dim = -1)\n",
    "            \n",
    "            decoded.append(word)\n",
    "        \n",
    "        return torch.stack(decoded, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "f08a5462-92a0-4cd2-8625-2fe59fb1b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "b4ccc0c1-047b-412c-9d9f-f4e0223d0db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "decoder = LSTMDecoder(output_dim = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "f24ddf62-b2c9-4a22-bb96-6d59c818da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12\n",
    "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, collate_fn=collator, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b53b94dd-c3cb-4143-914c-f8ae6b15ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(train_loader):\n",
    "    waveforms = batch['waveforms']\n",
    "    sentence = batch['sentences']\n",
    "    melspecs = batch['melspecs']\n",
    "    \n",
    "    melspecs_lengths = batch['melspecs_lengths']\n",
    "    waveforms_lengths = batch['waveforms_lengths']\n",
    "    \n",
    "    melspecs = torch.transpose(melspecs, -1, -2) ## Changing to (batch, channel, time, n_mels) to (batch, channel, n_mels, time)\n",
    "    \n",
    "    encoded_x = encoder.forward(melspecs, melspecs_lengths)\n",
    "    \n",
    "    decoded_y = decoder.forward(encoded_x)\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781879f1-e7ca-4f08-9df8-0d7305c1f301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Speech",
   "language": "python",
   "name": "speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
