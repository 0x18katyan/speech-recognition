{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a05c6a0-a5b7-459d-95cb-810afe750ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.functional as TAF\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from utils.dataset import CommonVoice\n",
    "from utils.audio_utils import plot_waveform, play_audio\n",
    "from utils.batch_utls import Collator\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78560c6e-e412-45ff-8282-58faf5284490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446aac2-89a2-4747-bb36-41fedc0ef545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pkbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6071366-8bd4-490f-800d-64e0439b3dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0 \n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f8f35-221a-44d3-8640-caa0fb67ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110dab0c-10b1-4665-8726-cc37bcfd6371",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d71108-5825-473e-9f11-6db91a6a86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc2acde-600e-4545-9207-76b48e5abe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetPATH = 'data/external/cv-corpus-8.0-2022-01-19/en/'\n",
    "clipsPATH = os.path.join(datasetPATH, 'clips')\n",
    "\n",
    "tokenizer_file = 'data/tokenizer/trained_tokenizer.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9d81a9-6042-486e-9864-71163a57929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: ##Check if tokenizer is defined\n",
    "    tokenizer\n",
    "\n",
    "except NameError as e: ## If tokenizer is not defined then initialize it\n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_file)\n",
    "\n",
    "finally:\n",
    "    special_tokens_dict = {'pad_token': '[PAD]',\n",
    "                       'sep_token': '[SEP]',\n",
    "                       'mask_token': '[MASK]'}\n",
    "    \n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    \n",
    "    blank_token = \"[PAD]\"\n",
    "    blank_token_id = tokenizer.vocab[blank_token]\n",
    "    \n",
    "    vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4230b8ef-817a-4449-a113-19dc2518c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CommonVoice(dataset_path = datasetPATH, split_type = 'train', tokenizer = tokenizer, out_channels = 1)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39c7d2-6f0a-4c26-a634-509801ff0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim: int = 128, \n",
    "                 num_heads: int = 4, \n",
    "                 ffn_dim: int = 128, \n",
    "                 num_layers: int = 4, \n",
    "                 depthwise_conv_kernel_size: int = 31, \n",
    "                 dropout: float = 0.3,\n",
    "                 **args):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.model = torchaudio.models.Conformer(input_dim = input_dim,\n",
    "                                                 num_heads = num_heads,\n",
    "                                                 ffn_dim = ffn_dim,\n",
    "                                                 num_layers = num_layers,\n",
    "                                                 depthwise_conv_kernel_size = depthwise_conv_kernel_size,\n",
    "                                                 dropout = dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, x_len: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        x, _ = self.model.forward(x, x_len)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7d3a6-3d45-4011-8035-fb116f66e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim: int = 128, \n",
    "                 hidden_size: int = 256, \n",
    "                 num_layers: int = 2, \n",
    "                 bidirectional: bool = False, \n",
    "                 output_dim: int = None, \n",
    "                 padding_idx: int = None, \n",
    "                 **args):\n",
    "        \n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        \n",
    "        if output_dim == None:\n",
    "            raise ValueError(\"Please specify the output size of the vocab.\")\n",
    "            \n",
    "        directions = 2 if bidirectional == True else 1\n",
    "            \n",
    "        self.model = nn.GRU(input_size = input_dim, hidden_size = hidden_size, num_layers = num_layers, batch_first = True)\n",
    "        self.ffn = nn.Linear(in_features = hidden_size * directions, out_features = output_dim)\n",
    "                                \n",
    "    def forward(self, x: torch.Tensor, hidden_state: torch.Tensor = None) -> torch.Tensor: \n",
    "        \"\"\"\n",
    "        Hidden state is needed, either in the form of encoder_hidden_state or decoder_hidden_state\n",
    "        \"\"\"\n",
    "        \n",
    "        if hidden_state == None:\n",
    "            outputs, hidden_state = self.model(x, hidden_state)\n",
    "        \n",
    "        else:\n",
    "            outputs, hidden_state = self.model(x, hidden_state)\n",
    "        \n",
    "        outputs = F.glu(self.ffn(outputs))\n",
    "\n",
    "        return outputs, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f105b7a-a4a4-422d-b956-280b09adc5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_input_dim: int = 128,\n",
    "                encoder_num_heads: int = 4, \n",
    "                encoder_ffn_dim: int = 128, \n",
    "                encoder_num_layers: int = 4, \n",
    "                encoder_depthwise_conv_kernel_size: int = 31, \n",
    "                decoder_hidden_size:int = 128,\n",
    "                decoder_num_layers: int = 2,\n",
    "                bidirectional_decoder: bool = False,\n",
    "                vocab_size: int = None,\n",
    "                padding_idx: int = None,\n",
    "                sos_token_id: int = None):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(input_dim = encoder_input_dim,\n",
    "                              num_heads = encoder_num_heads,\n",
    "                              ffn_dim = encoder_ffn_dim,\n",
    "                              depthwise_conv_kernel_size = encoder_depthwise_conv_kernel_size)\n",
    "        \n",
    "        self.decoder = LSTMDecoder(input_dim = encoder_input_dim,\n",
    "                                  hidden_size = decoder_hidden_size,\n",
    "                                  num_layers = decoder_num_layers,\n",
    "                                  bidirectional = bidirectional_decoder,\n",
    "                                  output_dim = vocab_size)\n",
    "        \n",
    "        self.sos_token_id = sos_token_id\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, x_lens: torch.Tensor):\n",
    "        \n",
    "        decoded = []\n",
    "        \n",
    "        bsz, msl, hdz = x.shape ##batch_size, max sequence length, hidden dimension size\n",
    "\n",
    "        encoder_outputs = self.encoder(x, x_lens)\n",
    "                \n",
    "        decoder_inputs = encoder_outputs\n",
    "        \n",
    "        ## Start with the <sos> token\n",
    "        x = torch.LongTensor([self.sos_token_id]).repeat(bsz).reshape(bsz, 1).to(device)\n",
    "\n",
    "        for t in range(msl):\n",
    "            \n",
    "            if t == 0:\n",
    "                decoder_output, decoder_hidden_state = self.decoder(x = decoder_inputs)            \n",
    "            else:\n",
    "                decoder_output, decoder_hidden_state = self.decoder(x = decoder_inputs, hidden_state = decoder_hidden_state)\n",
    "            \n",
    "            word = F.log_softmax(decoder_output, dim = -1) ## have to do log_softmax for CTC Loss\n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            \n",
    "            x = topv.squeeze().detach()\n",
    "            \n",
    "            decoded.append(topv)\n",
    "            \n",
    "        return encoder_outputs, torch.stack(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5065e96-6ddb-464a-bee0-0f9a8620e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'encoder_input_dim': 128,\n",
    "    'encoder_num_heads': 4,\n",
    "    'encoder_ffn_dim': 128,\n",
    "    'encoder_num_layers': 4,\n",
    "    'decoder_num_layers': 1,\n",
    "    'decoder_hidden_size': 64,\n",
    "    'padding_idx': tokenizer.pad_token_id,\n",
    "    'sos_token_id': tokenizer.bos_token_id\n",
    "}\n",
    "\n",
    "collator = Collator(tokenizer)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                          batch_size = BATCH_SIZE, \n",
    "                          collate_fn=collator, \n",
    "                          shuffle=True, \n",
    "                          pin_memory = True, \n",
    "                          num_workers = 6, \n",
    "                          worker_init_fn = collator.seed_worker, \n",
    "                          generator = g)\n",
    "\n",
    "fp16 = False\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4908b0-ab46-440c-b018-278c52414abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(**model_params).to(device)\n",
    "decoder = nn.Linear(model_params['encoder_input_dim'], out_features= vocab_size, bias = False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe92141-a09d-48b5-9255-ef5771531510",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CTC loss should be computed after the encoder outputs the probabilities\n",
    "\n",
    "## Decoding part is usually decoupled from encoding part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b2157-d7ea-4a3c-b8e2-b036fa743f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_log_dir = 'logs/'\n",
    "\n",
    "start_time = datetime.now()\n",
    "start_time_fmt = start_time.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "\n",
    "run_log_dir = os.path.join(base_log_dir, start_time_fmt)\n",
    "\n",
    "writer = SummaryWriter(log_dir=run_log_dir, comment = 'first_try_custom_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4e160-0d83-41ce-ab98-df0f8c76043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "num_batches = len(train_loader)\n",
    "\n",
    "enc_optim = torch.optim.AdamW(encoder.parameters(), lr = 3e-5)\n",
    "dec_optim = torch.optim.AdamW(decoder.parameters(), lr = 3e-5)\n",
    "\n",
    "enc_scheduler = ReduceLROnPlateau(enc_optim, mode = 'min')\n",
    "dec_scheduler = ReduceLROnPlateau(dec_optim, mode = 'min')\n",
    "\n",
    "loss_fn = nn.CTCLoss(blank = tokenizer.pad_token_id)\n",
    "\n",
    "n_iter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    kbar = pkbar.Kbar(target = num_batches, epoch = epoch, num_epochs=EPOCHS, width = 8, always_stateful=False)\n",
    "    \n",
    "    enc_optim.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        # waveforms = batch['waveforms']\n",
    "        # waveforms_lengths = batch['waveforms_lengths']\n",
    "        \n",
    "        sentences = batch['sentences'].to(device)\n",
    "        sentence_lengths = batch['sentence_lengths'].to(device, dtype= torch.int32)\n",
    "\n",
    "        melspecs = batch['melspecs'].to(device)\n",
    "        melspecs_lengths = batch['melspecs_lengths'].to(device, dtype= torch.int32)\n",
    "        \n",
    "        melspecs = torch.transpose(melspecs, -1, -2) ## Changing to (batch, channel, time, n_mels) from (batch, channel, n_mels, time)\n",
    "        \n",
    "        if fp16:\n",
    "            \n",
    "            with autocast():\n",
    "\n",
    "                encoder_outputs = encoder(melspecs, melspecs_lengths)\n",
    "                encoder_outputs = encoder_outputs.transpose(1, 0)\n",
    "\n",
    "                decoder_outputs = decoder(encoder_outputs)\n",
    "                decoder_outputs = F.log_softmax(decoder_outputs, dim = -1)\n",
    "                \n",
    "                decoder_outputs = decoder_outputs.to(dtype = torch.float32)\n",
    "                \n",
    "                melspecs_lengths = batch['melspecs_lengths'].to(dtype= torch.int32)\n",
    "                sentence_lengths = batch['sentence_lengths'].to(dtype= torch.int32)\n",
    "\n",
    "                ## CTC loss requires int32 and (T, B, L) shape for log_probabilities from decoder\n",
    "\n",
    "                ctc_loss = loss_fn(log_probs = decoder_outputs, \n",
    "                                   targets = sentences, \n",
    "                                   input_lengths = melspecs_lengths, \n",
    "                                   target_lengths=sentence_lengths)\n",
    "                \n",
    "                scaler.scale(ctc_loss).backward()\n",
    "                scaler.step(enc_optim)\n",
    "                scaler.step(dec_optim)\n",
    "                scaler.update()\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            encoder_outputs = encoder(melspecs, melspecs_lengths)\n",
    "            encoder_outputs = encoder_outputs.transpose(1, 0)\n",
    "\n",
    "            decoder_outputs = decoder(encoder_outputs)\n",
    "            decoder_outputs = F.log_softmax(decoder_outputs, dim = -1)\n",
    "\n",
    "            ctc_loss = loss_fn(log_probs = decoder_outputs, \n",
    "                               targets = sentences, \n",
    "                               input_lengths = melspecs_lengths, \n",
    "                               target_lengths=sentence_lengths)\n",
    "\n",
    "            ctc_loss.backward()\n",
    "\n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "            \n",
    "        enc_scheduler.step()\n",
    "        dec_scheduler.step()\n",
    "        \n",
    "        writer.add_scalar('CTC Loss/train', ctc_loss.detach().cpu().item(), (idx * (epoch + 1)))\n",
    "        \n",
    "        kbar.update(idx, values = [(\"ctc_loss\", ctc_loss.detach().cpu().item())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c22c6-5228-47cf-9036-6fa49f804e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_data.__getitem__(1)\n",
    "melspec = sample['melspec'].to(device)\n",
    "melspec_len = torch.Tensor([melspec.shape[-1]]).to(device)\n",
    "\n",
    "melspec= melspec.unsqueeze(0)\n",
    "melspec = melspec.transpose(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882552f9-37fa-4b29-9b6b-4e5378deaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_preds = encoder(melspec, melspec_len)\n",
    "    \n",
    "    y_preds = y_preds.transpose(1, 0)\n",
    "\n",
    "    y_preds = decoder(y_preds)\n",
    "    y_preds = F.log_softmax(y_preds, dim = -1)\n",
    "    \n",
    "    y_preds = y_preds.transpose(0,1)\n",
    "\n",
    "    y_preds = y_preds.argmax(dim = -1)\n",
    "    y_preds = torch.unique_consecutive(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1958bd42-ded9-4e21-b145-2c1419013eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412e79f-b46e-4234-a639-fbd22e4f6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae4825-6314-434b-96b7-831ff57721cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Speech",
   "language": "python",
   "name": "speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
