{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a05c6a0-a5b7-459d-95cb-810afe750ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from torch.optim import RAdam\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, PackedSequence\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.functional as TAF\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from utils.dataset import CommonVoice\n",
    "from utils.audio_utils import plot_waveform, play_audio\n",
    "from utils.batch_utils import Collator\n",
    "from utils.tokenizer import get_tokenizer\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bccc5281-1c6f-490f-b2de-72d7c9187fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f237251-1e2f-402f-a0b0-995c07bf1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.misc import get_summary, get_writer\n",
    "from utils.grad_flow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78560c6e-e412-45ff-8282-58faf5284490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2446aac2-89a2-4747-bb36-41fedc0ef545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pkbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6071366-8bd4-490f-800d-64e0439b3dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0 \n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3f8f35-221a-44d3-8640-caa0fb67ba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "0.11.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110dab0c-10b1-4665-8726-cc37bcfd6371",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d71108-5825-473e-9f11-6db91a6a86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc2acde-600e-4545-9207-76b48e5abe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'data/external/cv-corpus-8.0-2022-01-19/en/'\n",
    "\n",
    "tokenizer_file = 'data/tokenizer/trained_tokenizer.json'\n",
    "\n",
    "trimmed_train_path = 'data/internal/sample_train.tsv'\n",
    "# trimmed_train_path = 'data/internal/train_trimmed.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "277f89b6-6c88-4763-a927-e2f8b8d1b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, blank_token = get_tokenizer(tokenizer_file_path=tokenizer_file)\n",
    "\n",
    "blank_token_id = tokenizer.vocab[blank_token]\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4230b8ef-817a-4449-a113-19dc2518c9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    CommonVoice Dataset\n",
      "    -------------------\n",
      "    \n",
      "    Loading None.tsv from /home/ashim/Projects/DeepSpeech/data/external/cv-corpus-8.0-2022-01-19/en directory.\n",
      "        \n",
      "    Number of Examples: 4192\n",
      "    \n",
      "    Args:\n",
      "        Sampling Rate: 16000\n",
      "        Output Channels: 1\n",
      "    \n",
      "\n",
      "    CommonVoice Dataset\n",
      "    -------------------\n",
      "    \n",
      "    Loading dev.tsv from /home/ashim/Projects/DeepSpeech/data/external/cv-corpus-8.0-2022-01-19/en directory.\n",
      "        \n",
      "    Number of Examples: 16326\n",
      "    \n",
      "    Args:\n",
      "        Sampling Rate: 16000\n",
      "        Output Channels: 1\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "train_data = CommonVoice(dataset_dir = dataset_dir, subset_path = trimmed_train_path, tokenizer = tokenizer, out_channels = 1)\n",
    "dev_data = CommonVoice(dataset_dir = dataset_dir, subset_name = 'dev', tokenizer = tokenizer, out_channels = 1)\n",
    "\n",
    "print(train_data)\n",
    "\n",
    "print(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a39c7d2-6f0a-4c26-a634-509801ff0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 encoder_input_dim: int = 80, \n",
    "                 num_heads: int = 4, \n",
    "                 ffn_dim: int = 80, \n",
    "                 num_layers: int = 4, \n",
    "                 depthwise_conv_kernel_size: int = 31, \n",
    "                 dropout: float = 0.3,\n",
    "                 **args):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conformer = torchaudio.models.Conformer(input_dim = encoder_input_dim,\n",
    "                                                     num_heads = num_heads,\n",
    "                                                     ffn_dim = ffn_dim,\n",
    "                                                     num_layers = num_layers,\n",
    "                                                     depthwise_conv_kernel_size = depthwise_conv_kernel_size,\n",
    "                                                     dropout = dropout)\n",
    "                \n",
    "    def forward(self, x: torch.Tensor, x_lens: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        x, x_lens = self.conformer.forward(x, x_lens)\n",
    "        \n",
    "        return x, x_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66d7d3a6-3d45-4011-8035-fb116f66e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 decoder_input_dim: int = 80, \n",
    "                 decoder_hidden_size: int = 256, \n",
    "                 num_layers: int = 1, \n",
    "                 bidirectional: bool = False, \n",
    "                 output_dim: int = None, \n",
    "                 **args):\n",
    "        \n",
    "        super(RNNDecoder, self).__init__()\n",
    "        \n",
    "        if output_dim == None:\n",
    "            raise ValueError(\"Please specify the output size of the vocab.\")\n",
    "            \n",
    "        directions = 2 if bidirectional == True else 1\n",
    "            \n",
    "        self.model = nn.GRU(input_size = decoder_input_dim, hidden_size = decoder_hidden_size, num_layers = num_layers, batch_first = False)\n",
    "        self.ffn = nn.Linear(in_features = decoder_hidden_size * directions, out_features = output_dim)\n",
    "                                \n",
    "    def forward(self, x: torch.Tensor, hidden_state: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]: \n",
    "        \"\"\"\n",
    "        Hidden state is needed, either in the form of encoder_hidden_state or decoder_hidden_state\n",
    "        \"\"\"\n",
    "        \n",
    "        if hidden_state == None:\n",
    "            outputs, hidden_state = self.model(x)\n",
    "        \n",
    "        else:\n",
    "            outputs, hidden_state = self.model(x, hidden_state)\n",
    "        \n",
    "        if isinstance(packed_encoder_outputs, PackedSequence):\n",
    "            outputs, _ = pad_packed_sequence(outputs)\n",
    "        \n",
    "        outputs = self.ffn(outputs)\n",
    "        \n",
    "        return outputs, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f105b7a-a4a4-422d-b956-280b09adc5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_input_dim: int = 80,\n",
    "                encoder_num_heads: int = 4, \n",
    "                encoder_ffn_dim: int = 144, \n",
    "                encoder_num_layers: int = 16, \n",
    "                encoder_depthwise_conv_kernel_size: int = 31, \n",
    "                decoder_hidden_size:int = 80,\n",
    "                decoder_num_layers: int = 2,\n",
    "                bidirectional_decoder: bool = False,\n",
    "                vocab_size: int = None,\n",
    "                padding_idx: int = None,\n",
    "                sos_token_id: int = None):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(input_dim = encoder_input_dim,\n",
    "                              num_heads = encoder_num_heads,\n",
    "                              ffn_dim = encoder_ffn_dim,\n",
    "                              depthwise_conv_kernel_size = encoder_depthwise_conv_kernel_size)\n",
    "        \n",
    "        self.decoder = RNNDecoder(input_dim = encoder_input_dim,\n",
    "                                  hidden_size = decoder_hidden_size,\n",
    "                                  num_layers = decoder_num_layers,\n",
    "                                  bidirectional = bidirectional_decoder,\n",
    "                                  output_dim = vocab_size)\n",
    "        \n",
    "        self.sos_token_id = sos_token_id\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, x_lens: torch.Tensor):\n",
    "        \n",
    "        decoded = []\n",
    "        \n",
    "        bsz, msl, hdz = x.shape ##batch_size, max sequence length, hidden dimension size\n",
    "\n",
    "        encoder_outputs = self.encoder(x, x_lens)\n",
    "                \n",
    "        decoder_inputs = encoder_outputs\n",
    "        \n",
    "        ## Start with the <sos> token\n",
    "        x = torch.LongTensor([self.sos_token_id]).repeat(bsz).reshape(bsz, 1).to(device)\n",
    "\n",
    "        for t in range(msl):\n",
    "            \n",
    "            if t == 0:\n",
    "                decoder_output, decoder_hidden_state = self.decoder(x = decoder_inputs)            \n",
    "            else:\n",
    "                decoder_output, decoder_hidden_state = self.decoder(x = decoder_inputs, hidden_state = decoder_hidden_state)\n",
    "            \n",
    "            word = F.log_softmax(decoder_output, dim = -1) ## have to do log_softmax for CTC Loss\n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            \n",
    "            x = topv.squeeze().detach()\n",
    "            \n",
    "            decoded.append(topv)\n",
    "            \n",
    "        return encoder_outputs, torch.stack(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5065e96-6ddb-464a-bee0-0f9a8620e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'encoder_input_dim': 80,\n",
    "    'encoder_num_heads': 4,\n",
    "    'encoder_ffn_dim': 144,\n",
    "    'encoder_num_layers': 16,\n",
    "    'decoder_input_dim': 80,\n",
    "    'decoder_hidden_size': 256,\n",
    "    'decoder_num_layers': 1,\n",
    "    'decoder_hidden_size': 320,\n",
    "    'padding_idx': tokenizer.pad_token_id,\n",
    "    'sos_token_id': tokenizer.bos_token_id,\n",
    "    'vocab_size': vocab_size\n",
    "}\n",
    "\n",
    "collator = Collator(tokenizer)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                          batch_size = BATCH_SIZE, \n",
    "                          collate_fn=collator, \n",
    "                          shuffle=True, \n",
    "                          pin_memory = True, \n",
    "                          num_workers = 8, \n",
    "                          worker_init_fn = collator.seed_worker, \n",
    "                          generator = g)\n",
    "\n",
    "fp16 = False\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de4908b0-ab46-440c-b018-278c52414abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(**model_params).to(device)\n",
    "decoder = RNNDecoder(**model_params, output_dim = vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d98a8d1d-6e5b-4f9b-8be0-cee5d214c87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================================================\n",
       "Layer (type:depth-idx)                                            Output Shape     Param #\n",
       "=================================================================================================\n",
       "Encoder                                                           --               --\n",
       "├─Conformer: 1-1                                                  --               --\n",
       "│    └─ModuleList: 2-1                                            --               --\n",
       "│    │    └─ConformerLayer: 3-1                                   [92, 32, 80]     74,800\n",
       "│    │    └─ConformerLayer: 3-2                                   [92, 32, 80]     74,800\n",
       "│    │    └─ConformerLayer: 3-3                                   [92, 32, 80]     74,800\n",
       "│    │    └─ConformerLayer: 3-4                                   [92, 32, 80]     74,800\n",
       "=================================================================================================\n",
       "Total params: 221,440\n",
       "Trainable params: 221,440\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 268.89\n",
       "=================================================================================================\n",
       "Input size (MB): 0.94\n",
       "Forward/backward pass size (MB): 105.51\n",
       "Params size (MB): 0.89\n",
       "Estimated Total Size (MB): 107.34\n",
       "================================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_summary(encoder, dataloader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebe92141-a09d-48b5-9255-ef5771531510",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CTC loss should be computed after the encoder outputs the probabilities\n",
    "\n",
    "## Decoding part is usually decoupled from encoding part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "370b2157-d7ea-4a3c-b8e2-b036fa743f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_log_dir = 'logs/'\n",
    "writer = get_writer(base_log_dir=base_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b552416d-b9d6-4921-9594-d0ab46eabf32",
   "metadata": {},
   "source": [
    "## BATCH_FIRST = FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91294410-15bc-4060-9fae-27154d4516d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NORM = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6175e987-149c-4617-ad32-a13756bbaa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "lr = 5.0  # learning rate\n",
    "\n",
    "num_batches = len(train_loader)\n",
    "\n",
    "# enc_optim = torch.optim.AdamW(encoder.parameters(), lr = lr)\n",
    "# dec_optim = torch.optim.AdamW(decoder.parameters(), lr = lr)\n",
    "\n",
    "# enc_optim = RAdam(encoder.parameters(), lr = lr)\n",
    "# dec_optim = RAdam(decoder.parameters(), lr = lr)\n",
    "\n",
    "enc_optim = torch.optim.SGD(encoder.parameters(), lr = lr)\n",
    "dec_optim = torch.optim.SGD(decoder.parameters(), lr = lr)\n",
    "\n",
    "enc_scheduler_plateau = ReduceLROnPlateau(enc_optim, mode = 'min', patience = 2)\n",
    "dec_scheduler_plateau = ReduceLROnPlateau(dec_optim, mode = 'min', patience = 2)\n",
    "\n",
    "\n",
    "enc_scheduler_stepLR = torch.optim.lr_scheduler.StepLR(enc_optim, 1.0, gamma=0.95)\n",
    "dec_scheduler_stepLR = torch.optim.lr_scheduler.StepLR(dec_optim, 1.0, gamma=0.95)\n",
    "\n",
    "loss_fn = nn.CTCLoss(blank = blank_token_id, zero_infinity = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5c4e160-0d83-41ce-ab98-df0f8c76043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 7.2308\n",
      "\n",
      "Epoch: 2/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0799\n",
      "\n",
      "Epoch: 3/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0797\n",
      "\n",
      "Epoch: 4/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0799\n",
      "\n",
      "Epoch: 5/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0798\n",
      "\n",
      "Epoch: 6/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0801\n",
      "\n",
      "Epoch: 7/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0798\n",
      "\n",
      "Epoch: 8/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0801\n",
      "\n",
      "Epoch: 9/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0798\n",
      "\n",
      "Epoch: 10/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0795\n",
      "\n",
      "Epoch: 11/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0799\n",
      "\n",
      "Epoch: 12/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0802\n",
      "\n",
      "Epoch: 13/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0801\n",
      "\n",
      "Epoch: 14/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0796\n",
      "\n",
      "Epoch: 15/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0794\n",
      "\n",
      "Epoch: 16/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0797\n",
      "\n",
      "Epoch: 17/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0792\n",
      "\n",
      "Epoch: 18/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0791\n",
      "\n",
      "Epoch: 19/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0794\n",
      "\n",
      "Epoch: 20/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0795\n",
      "\n",
      "Epoch: 21/50\n",
      "130/131 [======>.] - ETA: 0s - ctc_loss: 6.0799\n",
      "\n",
      "Epoch: 22/50\n",
      "125/131 [======>.] - ETA: 2s - ctc_loss: 6.0820"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m dec_scheduler_stepLR\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCTC Loss/train\u001b[39m\u001b[38;5;124m'\u001b[39m, ctc_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem(), n_iter)\n\u001b[0;32m---> 60\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAverage Gradients/Encoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_grad_flow_fig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_figure(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Gradients/Decoder\u001b[39m\u001b[38;5;124m'\u001b[39m, dec_grad_flow_fig, global_step \u001b[38;5;241m=\u001b[39m n_iter, close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m kbar\u001b[38;5;241m.\u001b[39mupdate(idx, values \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctc_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, ctc_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem())])\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py:658\u001b[0m, in \u001b[0;36mSummaryWriter.add_figure\u001b[0;34m(self, tag, figure, global_step, close, walltime)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_image(tag, figure_to_image(figure, close), global_step, walltime, dataformats\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNCHW\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_image(tag, \u001b[43mfigure_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose\u001b[49m\u001b[43m)\u001b[49m, global_step, walltime, dataformats\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCHW\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/torch/utils/tensorboard/_utils.py:35\u001b[0m, in \u001b[0;36mfigure_to_image\u001b[0;34m(figures, close)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(images)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mrender_to_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/torch/utils/tensorboard/_utils.py:22\u001b[0m, in \u001b[0;36mfigure_to_image.<locals>.render_to_rgb\u001b[0;34m(figure)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender_to_rgb\u001b[39m(figure):\n\u001b[1;32m     21\u001b[0m     canvas \u001b[38;5;241m=\u001b[39m plt_backend_agg\u001b[38;5;241m.\u001b[39mFigureCanvasAgg(figure)\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(canvas\u001b[38;5;241m.\u001b[39mbuffer_rgba(), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m     24\u001b[0m     w, h \u001b[38;5;241m=\u001b[39m figure\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mget_width_height()\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:436\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m RendererAgg\u001b[38;5;241m.\u001b[39mlock, \\\n\u001b[1;32m    434\u001b[0m      (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    435\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/artist.py:73\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 73\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     75\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/artist.py:50\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/figure.py:2810\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2807\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   2809\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 2810\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[1;32m   2814\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/artist.py:50\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/axes/_base.py:3082\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3079\u001b[0m         a\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m   3080\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n\u001b[0;32m-> 3082\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3085\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3086\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/artist.py:50\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/axis.py:1159\u001b[0m, in \u001b[0;36mAxis.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m renderer\u001b[38;5;241m.\u001b[39mopen_group(\u001b[38;5;18m__name__\u001b[39m, gid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_gid())\n\u001b[1;32m   1158\u001b[0m ticks_to_draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_ticks()\n\u001b[0;32m-> 1159\u001b[0m ticklabelBoxes, ticklabelBoxes2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tick_bboxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticks_to_draw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks_to_draw:\n\u001b[1;32m   1163\u001b[0m     tick\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/axis.py:1085\u001b[0m, in \u001b[0;36mAxis._get_tick_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_tick_bboxes\u001b[39m(\u001b[38;5;28mself\u001b[39m, ticks, renderer):\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;124;03m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ([tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1086\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[1;32m   1087\u001b[0m             [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1088\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/axis.py:1085\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_tick_bboxes\u001b[39m(\u001b[38;5;28mself\u001b[39m, ticks, renderer):\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;124;03m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ([\u001b[43mtick\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1086\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[1;32m   1087\u001b[0m             [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1088\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/text.py:910\u001b[0m, in \u001b[0;36mText.get_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot get window extent w/o renderer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m--> 910\u001b[0m     bbox, info, descent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_unitless_position()\n\u001b[1;32m    912\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transform()\u001b[38;5;241m.\u001b[39mtransform((x, y))\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/text.py:317\u001b[0m, in \u001b[0;36mText._get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    315\u001b[0m clean_line, ismath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_math(line)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_line:\n\u001b[0;32m--> 317\u001b[0m     w, h, d \u001b[38;5;241m=\u001b[39m \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_width_height_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclean_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fontproperties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mismath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mismath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    320\u001b[0m     w \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m=\u001b[39m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:270\u001b[0m, in \u001b[0;36mRendererAgg.get_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    268\u001b[0m flags \u001b[38;5;241m=\u001b[39m get_hinting_flag()\n\u001b[1;32m    269\u001b[0m font \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_agg_font(prop)\n\u001b[0;32m--> 270\u001b[0m \u001b[43mfont\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m w, h \u001b[38;5;241m=\u001b[39m font\u001b[38;5;241m.\u001b[39mget_width_height()  \u001b[38;5;66;03m# width and height of unrotated string\u001b[39;00m\n\u001b[1;32m    272\u001b[0m d \u001b[38;5;241m=\u001b[39m font\u001b[38;5;241m.\u001b[39mget_descent()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    kbar = pkbar.Kbar(target = num_batches, epoch = epoch, num_epochs=EPOCHS, width = 8, always_stateful=False)\n",
    "    \n",
    "    enc_optim.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        # waveforms = batch['waveforms']\n",
    "        # waveforms_lengths = batch['waveforms_lengths']\n",
    "        \n",
    "        sentences = batch['sentences'].to(device)\n",
    "        sentence_lengths = batch['sentence_lengths'].to(device, dtype= torch.int32)\n",
    "\n",
    "        melspecs = batch['melspecs'].to(device)\n",
    "        melspecs_lengths = batch['melspecs_lengths'].to(device, dtype= torch.int32)\n",
    "        \n",
    "        melspecs = torch.transpose(melspecs, -1, -2) ## Changing to (batch, channel, time, n_mels) from (batch, channel, n_mels, time)\n",
    "        \n",
    "        encoder_outputs, encoder_output_lengths = encoder(melspecs, melspecs_lengths)\n",
    "        encoder_outputs = encoder_outputs.transpose(1, 0)\n",
    "\n",
    "        ##encoder_outputs: [seq_len, batch_size, hidden_dim]\n",
    "\n",
    "        ## Packing is done to ensure model doesn't take the [PAD] token into consideration\n",
    "\n",
    "        packed_encoder_outputs = pack_padded_sequence(encoder_outputs, \n",
    "                                          lengths = encoder_output_lengths.to(device = 'cpu', dtype=torch.int64), \n",
    "                                          batch_first = False, \n",
    "                                          enforce_sorted = False)\n",
    "\n",
    "        decoder_outputs, decoder_hidden_state = decoder(packed_encoder_outputs)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim = -1)\n",
    "\n",
    "        ctc_loss = loss_fn(log_probs = decoder_outputs, \n",
    "                           targets = sentences, \n",
    "                           input_lengths = melspecs_lengths, \n",
    "                           target_lengths=sentence_lengths)\n",
    "\n",
    "        ctc_loss.backward()\n",
    "        \n",
    "        ## Plot Gradients every 10 steps\n",
    "        if n_iter % 10 == 0:\n",
    "        \n",
    "            enc_grad_flow_fig = plot_grad_flow_v2(encoder.named_parameters())\n",
    "            dec_grad_flow_fig = plot_grad_flow_v2(decoder.named_parameters())\n",
    "\n",
    "        clip_grad_norm_(encoder.parameters(), max_norm = MAX_NORM)\n",
    "        clip_grad_norm_(decoder.parameters(), max_norm = MAX_NORM)\n",
    "\n",
    "        enc_optim.step()\n",
    "        dec_optim.step()\n",
    "\n",
    "        enc_scheduler_stepLR.step()\n",
    "        dec_scheduler_stepLR.step()\n",
    "                    \n",
    "        writer.add_scalar('CTC Loss/train', ctc_loss.detach().cpu().item(), n_iter)\n",
    "        writer.add_figure('Average Gradients/Encoder', enc_grad_flow_fig, global_step = n_iter, close = True)\n",
    "        writer.add_figure('Average Gradients/Decoder', dec_grad_flow_fig, global_step = n_iter, close = True)\n",
    "        \n",
    "        kbar.update(idx, values = [(\"ctc_loss\", ctc_loss.detach().cpu().item())])\n",
    "        \n",
    "        n_iter += 1\n",
    "    \n",
    "    ## At epoch end\n",
    "    \n",
    "    enc_scheduler_plateau.step(ctc_loss)\n",
    "    dec_scheduler_plateau.step(ctc_loss)\n",
    "\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad3c22c6-5228-47cf-9036-6fa49f804e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_data.__getitem__(1)\n",
    "melspec = sample['melspec'].to(device)\n",
    "melspec_len = torch.Tensor([melspec.shape[-1]]).to(device)\n",
    "\n",
    "melspec= melspec.unsqueeze(0)\n",
    "melspec = melspec.transpose(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "882552f9-37fa-4b29-9b6b-4e5378deaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_preds, y_lens = encoder(melspec, melspec_len)\n",
    "    \n",
    "    y_preds = y_preds.transpose(1, 0)\n",
    "    \n",
    "    packed_y_preds = pack_padded_sequence(y_preds, \n",
    "                                  lengths = y_lens.to(device = 'cpu', dtype=torch.int64), \n",
    "                                  batch_first = False, \n",
    "                                  enforce_sorted = False)\n",
    "\n",
    "    \n",
    "    y_preds, hidden_state = decoder(packed_y_preds)\n",
    "    y_preds = F.softmax(y_preds, dim = -1)\n",
    "    \n",
    "    y_preds = y_preds.transpose(0,1)\n",
    "\n",
    "    y_preds = y_preds.argmax(dim = -1)\n",
    "    y_preds = torch.unique_consecutive(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1958bd42-ded9-4e21-b145-2c1419013eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'His fishing boat is struck by lightning and explodes into pieces, burns and sinks.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d98f18bc-58c2-4a4b-80df-d2fb0eb3103d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c412e79f-b46e-4234-a639-fbd22e4f6e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[BLANK]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae4825-6314-434b-96b7-831ff57721cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Speech",
   "language": "python",
   "name": "speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
