{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a05c6a0-a5b7-459d-95cb-810afe750ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau, StepLR\n",
    "from torch.optim import RAdam\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, PackedSequence\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "from utils.model import *\n",
    "from utils.attention import BadhanauAttention\n",
    "\n",
    "from utils.dataset import CommonVoice\n",
    "from utils.audio_utils import plot_waveform, play_audio\n",
    "from utils.batch_utils import Collator\n",
    "from utils.tokenizer import get_tokenizer\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bccc5281-1c6f-490f-b2de-72d7c9187fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f237251-1e2f-402f-a0b0-995c07bf1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.misc import get_summary, get_writer\n",
    "from utils.grad_flow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2446aac2-89a2-4747-bb36-41fedc0ef545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pkbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6071366-8bd4-490f-800d-64e0439b3dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0 \n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110dab0c-10b1-4665-8726-cc37bcfd6371",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc2acde-600e-4545-9207-76b48e5abe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'data/external/cv-corpus-8.0-2022-01-19/en/'\n",
    "\n",
    "tokenizer_file = 'data/tokenizer/trained_tokenizer.json'\n",
    "\n",
    "# trimmed_train_path = 'data/internal/sample_train.tsv'\n",
    "trimmed_train_path = 'data/internal/train_trimmed.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "277f89b6-6c88-4763-a927-e2f8b8d1b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(tokenizer_file_path=tokenizer_file)\n",
    "\n",
    "blank_token_id = tokenizer.vocab[\"[BLANK]\"]\n",
    "bos_token_id = tokenizer.vocab[\"[BOS]\"]\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4230b8ef-817a-4449-a113-19dc2518c9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    CommonVoice Dataset\n",
      "    -------------------\n",
      "    \n",
      "    Loading None.tsv from /home/ashim/Projects/DeepSpeech/data/external/cv-corpus-8.0-2022-01-19/en directory.\n",
      "        \n",
      "    Number of Examples: 864308\n",
      "    \n",
      "    Args:\n",
      "        Sampling Rate: 16000\n",
      "        Output Channels: 1\n",
      "    \n",
      "\n",
      "    CommonVoice Dataset\n",
      "    -------------------\n",
      "    \n",
      "    Loading dev.tsv from /home/ashim/Projects/DeepSpeech/data/external/cv-corpus-8.0-2022-01-19/en directory.\n",
      "        \n",
      "    Number of Examples: 16326\n",
      "    \n",
      "    Args:\n",
      "        Sampling Rate: 16000\n",
      "        Output Channels: 1\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "train_data = CommonVoice(dataset_dir = dataset_dir, subset_path = trimmed_train_path, tokenizer = tokenizer, out_channels = 1)\n",
    "# train_data = CommonVoice(dataset_dir = dataset_dir, subset_name = 'train', tokenizer = tokenizer, out_channels = 1)\n",
    "\n",
    "dev_data = CommonVoice(dataset_dir = dataset_dir, subset_name = 'dev', tokenizer = tokenizer, out_channels = 1)\n",
    "\n",
    "print(train_data)\n",
    "\n",
    "print(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbeb0b01-e6a7-4d25-a816-f00c0c4951db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'encoder_input_size': 80,\n",
    "    'conformer_num_heads': 4,\n",
    "    'conformer_ffn_size': 144,\n",
    "    'conformer_num_layers': 16,\n",
    "    'conformer_conv_kernel_size': 31,\n",
    "    'encoder_rnn_hidden_size': 256,\n",
    "    'encoder_rnn_num_layers': 1,\n",
    "    'encoder_rnn_bidirectional': False,\n",
    "    'decoder_embedding_size': 300,\n",
    "    'decoder_hidden_size': 256,\n",
    "    'decoder_num_layers': 1,\n",
    "    'decoder_bidirectional': False,\n",
    "    'decoder_attn_size': 84,\n",
    "    'dropout': 0.3,\n",
    "    'padding_idx': tokenizer.pad_token_id,\n",
    "    'sos_token_id': tokenizer.bos_token_id,\n",
    "    'vocab_size': vocab_size,\n",
    "    'batch_first': True,\n",
    "    'device': device,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5065e96-6ddb-464a-bee0-0f9a8620e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(tokenizer)\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                          batch_size = BATCH_SIZE, \n",
    "                          collate_fn=collator, \n",
    "                          shuffle=True, \n",
    "                          pin_memory = True, \n",
    "                          num_workers = 6, \n",
    "                          worker_init_fn = collator.seed_worker, \n",
    "                          generator = g)\n",
    "\n",
    "fp16 = False\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64596bb5-5ab5-4bab-8f4d-b8fdc3b2ec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashim/miniconda3/envs/speech/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = Model(**model_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d98a8d1d-6e5b-4f9b-8be0-cee5d214c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_summary(encoder, dataloader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebe92141-a09d-48b5-9255-ef5771531510",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CTC loss should be computed after the encoder outputs the probabilities\n",
    "\n",
    "## Decoding part is usually decoupled from encoding part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "370b2157-d7ea-4a3c-b8e2-b036fa743f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_log_dir = 'logs/'\n",
    "writer = get_writer(base_log_dir=base_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b552416d-b9d6-4921-9594-d0ab46eabf32",
   "metadata": {},
   "source": [
    "## BATCH_FIRST = FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91294410-15bc-4060-9fae-27154d4516d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NORM = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10bd7f24-9117-4dbc-aef4-f169695307e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_batch(batch: Dict, max_len: int = 50):\n",
    "    \n",
    "    melspec = batch['melspecs'].to(device).squeeze(0)\n",
    "    melspecs_lengths = batch['melspecs_lengths'].to(device, dtype = torch.int32)\n",
    "    \n",
    "    sentences = batch['sentences'].to(device)\n",
    "    sentence_lengths = torch.LongTensor(batch['sentence_lengths']).to(device=device)    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ##Change from [batch, feats, seq_len] to [batch, seq_len,featrs]\n",
    "        predicted_tensor = model.forward(melspec.permute(0,2,1), melspecs_lengths, sentences, sentence_lengths)\n",
    "        y_ids = predicted_tensor.argmax(dim = -1)\n",
    "        y_pred = torch.unique_consecutive(y_ids, dim = 1)\n",
    "        \n",
    "        y_pred = tokenizer.batch_decode(y_pred)\n",
    "    \n",
    "    y_true = tokenizer.batch_decode(sentences)\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e33cf11-0a82-4892-a25d-b7f6aea3b68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "try:\n",
    "    samples\n",
    "except NameError as e:\n",
    "    samples = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6175e987-149c-4617-ad32-a13756bbaa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "lr = 5.0  # learning rate\n",
    "\n",
    "num_batches = len(train_loader)\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "scheduler_plateau = ReduceLROnPlateau(optim, mode = 'min', patience = 2)\n",
    "\n",
    "scheduler_stepLR = StepLR(optim, 1.0, gamma=0.95)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "criterion = nn.NLLLoss(ignore_index=tokenizer.pad_token_id)\n",
    "# criterion = nn.CTCLoss(blank = tokenizer.vocab['[BLANK]'], \n",
    "#                      zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "679a0551-4b8c-4a79-b46c-53157937eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch: List[Dict[str, torch.Tensor]], n_iter: int, MAX_NORM: float = 0.5, plot_gradients: bool = True):\n",
    "        \n",
    "    sentences = batch['sentences'].to(device)\n",
    "    sentence_lengths = batch['sentence_lengths'].to(device, dtype = torch.int32)\n",
    "\n",
    "    melspecs = batch['melspecs'].to(device)\n",
    "    melspecs_lengths = batch['melspecs_lengths'].to(device, dtype = torch.int32)\n",
    "\n",
    "    melspecs = torch.transpose(melspecs, -1, -2) ## Changing to (batch, channel, time, n_mels) from (batch, channel, n_mels, time)\n",
    "\n",
    "    predicted_tensor = model.forward(melspecs, melspecs_lengths, sentences, sentence_lengths)\n",
    "    \n",
    "    y_pred = predicted_tensor.reshape(-1, vocab_size)\n",
    "    y_true = sentences[:, 1:].reshape(-1)\n",
    "    \n",
    "    loss = criterion( input = y_pred, target = y_true ) ##Skip the <sos> token from the targets        \n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    ## Plot Gradients every 10 steps\n",
    "    if n_iter % 10 == 0 and plot_gradients == True:\n",
    "\n",
    "        grad_flow_fig = plot_grad_flow_v2(model.named_parameters())\n",
    "    \n",
    "    else:\n",
    "        grad_flow_fig = None\n",
    "    \n",
    "    ## Gradient Clipping for exploding gradients\n",
    "    clip_grad_norm_(model.parameters(), max_norm = MAX_NORM)\n",
    "\n",
    "    ## Step the optimizers\n",
    "    optim.step()\n",
    "\n",
    "    ## Step the schedulers\n",
    "    scheduler_stepLR.step()\n",
    "\n",
    "    return loss.detach().cpu().item(), grad_flow_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71caa17-fa42-450a-b703-1392a41a53c4",
   "metadata": {},
   "source": [
    "## TODO\n",
    " Fix Grad Flow Figs\n",
    " Fix Per Sample Tensorboard Logging Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4acc4f9-c9e2-4234-8fa6-9961c01e082d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50\n",
      "torch.Size([256, 48, 1000])\n",
      "torch.Size([256])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (12288) to match target batch_size (256).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     12\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m     loss, grad_flow_fig \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m## Write how sample is being predicted \u001b[39;00m\n\u001b[1;32m     17\u001b[0m     _, sample_pred \u001b[38;5;241m=\u001b[39m predict_one_batch(samples)\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(batch, n_iter, MAX_NORM, plot_gradients)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_true\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m##Skip the <sos> token from the targets        \u001b[39;00m\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m## Plot Gradients every 10 steps\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/torch/nn/modules/loss.py:211\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/torch/nn/functional.py:2671\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2670\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (12288) to match target batch_size (256)."
     ]
    }
   ],
   "source": [
    "n_iter = 0\n",
    "\n",
    "y_true, _ = predict_one_batch(samples)\n",
    "writer.add_text('y_true sentence', y_true[0], global_step = n_iter)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    kbar = pkbar.Kbar(target = num_batches, epoch = epoch, num_epochs=EPOCHS, width = 8, always_stateful=False)\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss, grad_flow_fig = train_step(batch, n_iter, plot_gradients=False)\n",
    "        \n",
    "        ## Write how sample is being predicted \n",
    "        _, sample_pred = predict_one_batch(samples)\n",
    "        writer.add_text('y_pred sentence', sample_pred[0], global_step = n_iter)\n",
    "        \n",
    "        writer.add_scalar('CE Loss/train', loss, n_iter)\n",
    "        \n",
    "        if grad_flow_fig != None:\n",
    "            \n",
    "            writer.add_figure('Average Gradients/Model', grad_flow_fig, global_step = n_iter, close = True)\n",
    "\n",
    "        kbar.update(idx, values = [(\"loss\", loss)])\n",
    "\n",
    "        n_iter += 1\n",
    "    \n",
    "    ## At epoch end\n",
    "    \n",
    "    scheduler_plateau.step(loss)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23bdb18-08cf-4b89-bab6-112348dea975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Speech",
   "language": "python",
   "name": "speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
