{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c9692be-7a3d-40e2-bd87-cdbccbf1c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a05c6a0-a5b7-459d-95cb-810afe750ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.functional as TAF\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.data import load_sp_model, generate_sp_model\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "from utils.dataset import CommonVoice\n",
    "from utils.audio_utils import plot_waveform, play_audio\n",
    "from utils.batch_utls import Collator\n",
    "from utils.preprocess import Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af8815a9-c5bc-4d3d-815b-751972a47ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98367d89-5617-48c8-989d-e4b0a6e77006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78560c6e-e412-45ff-8282-58faf5284490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2446aac2-89a2-4747-bb36-41fedc0ef545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pkbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3f8f35-221a-44d3-8640-caa0fb67ba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "0.11.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110dab0c-10b1-4665-8726-cc37bcfd6371",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d71108-5825-473e-9f11-6db91a6a86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc2acde-600e-4545-9207-76b48e5abe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetPATH = 'data/external/cv-corpus-8.0-2022-01-19/en/'\n",
    "clipsPATH = os.path.join(datasetPATH, 'clips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a9d81a9-6042-486e-9864-71163a57929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: ##Check if tokenizer is defined\n",
    "    tokenizer\n",
    "except NameError as e: ## If tokenizer is not defined then initialize it\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "finally:\n",
    "    blank_token = \"<|blank|>\"\n",
    "    tokenizer.add_tokens(blank_token)\n",
    "    \n",
    "    blank_token_id = tokenizer.vocab[blank_token]\n",
    "    \n",
    "    vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4230b8ef-817a-4449-a113-19dc2518c9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    CommonVoice Dataset\n",
       "    -------------------\n",
       "    \n",
       "    Loading train.tsv from /home/ashim/Projects/DeepSpeech/data/external/cv-corpus-8.0-2022-01-19/en directory.\n",
       "        \n",
       "    Number of Examples: 864448\n",
       "    \n",
       "    Args:\n",
       "        Sampling Rate: 32000\n",
       "        Output Channels: 1\n",
       "    "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = CommonVoice(dataset_path = datasetPATH, split_type = 'train', tokenizer = tokenizer, out_channels = 1)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a39c7d2-6f0a-4c26-a634-509801ff0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim: int = 128, num_heads: int = 4, ffn_dim: int = 128, num_layers: int = 4, depthwise_conv_kernel_size: int = 31, dropout: float = 0.3):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.model = torchaudio.models.Conformer(input_dim = input_dim,\n",
    "                                                 num_heads = num_heads,\n",
    "                                                 ffn_dim = ffn_dim,\n",
    "                                                 num_layers = num_layers,\n",
    "                                                 depthwise_conv_kernel_size = depthwise_conv_kernel_size,\n",
    "                                                 dropout = dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, x_len: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        x, _ = self.model.forward(x, x_len)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d7d3a6-3d45-4011-8035-fb116f66e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim: int = 128, hidden_size: int = 256, num_layers: int = 2, bidirectional: bool = False, output_dim: int = None, padding_idx: int = None):\n",
    "        \n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        \n",
    "        if output_dim == None:\n",
    "            raise ValueError(\"Please specify the output size of the vocab.\")\n",
    "            \n",
    "        directions = 2 if bidirectional == True else 1\n",
    "            \n",
    "        self.model = nn.GRU(input_size = input_dim, hidden_size = hidden_size, num_layers = num_layers, batch_first = True)\n",
    "        self.ffn = nn.Linear(in_features = hidden_size * directions, out_features = output_dim)\n",
    "                                \n",
    "    def forward(self, x: torch.Tensor, hidden_state: torch.Tensor = None) -> torch.Tensor: \n",
    "        \"\"\"\n",
    "        Hidden state is needed, either in the form of encoder_hidden_state or decoder_hidden_state\n",
    "        \"\"\"\n",
    "        \n",
    "        if hidden_state == None:\n",
    "            outputs, hidden_state = self.model(x, hidden_state)\n",
    "        \n",
    "        else:\n",
    "            outputs, hidden_state = self.model(x, hidden_state)\n",
    "        \n",
    "        outputs = F.glu(self.ffn(outputs))\n",
    "\n",
    "        return outputs, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f105b7a-a4a4-422d-b956-280b09adc5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_input_dim: int = 128,\n",
    "                encoder_num_heads: int = 4, \n",
    "                encoder_ffn_dim: int = 128, \n",
    "                encoder_num_layers: int = 4, \n",
    "                encoder_depthwise_conv_kernel_size: int = 31, \n",
    "                decoder_hidden_size:int = 128,\n",
    "                decoder_num_layers: int = 2,\n",
    "                bidirectional_decoder: bool = False,\n",
    "                vocab_size: int = None,\n",
    "                padding_idx: int = None,\n",
    "                sos_token_id: int = None):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(input_dim = encoder_input_dim,\n",
    "                              num_heads = encoder_num_heads,\n",
    "                              ffn_dim = encoder_ffn_dim,\n",
    "                              depthwise_conv_kernel_size = encoder_depthwise_conv_kernel_size)\n",
    "        \n",
    "        self.decoder = LSTMDecoder(input_dim = encoder_input_dim,\n",
    "                                  hidden_size = decoder_hidden_size,\n",
    "                                  num_layers = decoder_num_layers,\n",
    "                                  bidirectional = bidirectional_decoder,\n",
    "                                  output_dim = vocab_size)\n",
    "        \n",
    "        self.sos_token_id = sos_token_id\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, x_lens: torch.Tensor):\n",
    "        \n",
    "        decoded = []\n",
    "        \n",
    "        bsz, msl, hdz = x.shape ##batch_size, max sequence length, hidden dimension size\n",
    "\n",
    "        encoder_outputs = self.encoder(x, x_lens)\n",
    "                \n",
    "        decoder_inputs = encoder_outputs\n",
    "        \n",
    "        ## Start with the <sos> token\n",
    "        x = torch.LongTensor([self.sos_token_id]).repeat(bsz).reshape(bsz, 1).to(device)\n",
    "\n",
    "        for t in range(msl):\n",
    "            \n",
    "            if t == 0:\n",
    "                decoder_output, decoder_hidden_state = self.decoder(x = decoder_inputs)            \n",
    "            else:\n",
    "                decoder_output, decoder_hidden_state = self.decoder(x = decoder_inputs, hidden_state = decoder_hidden_state)\n",
    "            \n",
    "            word = F.log_softmax(decoder_output, dim = -1) ## have to do log_softmax for CTC Loss\n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            \n",
    "            x = topv.squeeze().detach()\n",
    "            \n",
    "            decoded.append(topv)\n",
    "            \n",
    "        return encoder_outputs, torch.stack(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5065e96-6ddb-464a-bee0-0f9a8620e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'encoder_num_heads': 4,\n",
    "    'encoder_ffn_dim': 64,\n",
    "    'encoder_num_layers': 3,\n",
    "    'decoder_num_layers': 1,\n",
    "    'decoder_hidden_size': 64,\n",
    "    'padding_idx': tokenizer.pad_token_id,\n",
    "    'sos_token_id': tokenizer.bos_token_id\n",
    "}\n",
    "\n",
    "collator = Collator(tokenizer)\n",
    "BATCH_SIZE = 1\n",
    "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, collate_fn=collator, shuffle=True)\n",
    "\n",
    "fp16 = True\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebe92141-a09d-48b5-9255-ef5771531510",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CTC loss should be computed after the encoder outputs the probabilities\n",
    "\n",
    "## Decoding part is usually decoupled from encoding part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a4bd486-08e8-4f08-a881-3ffb5fc061fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(**model_params, vocab_size=vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5c4e160-0d83-41ce-ab98-df0f8c76043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1\n",
      "    14/864448 [..........] - ETA: 11:52:21 - ctc_loss: nan"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m bsz, msl, hdz \u001b[38;5;241m=\u001b[39m melspecs\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;66;03m##batch_size, max sequence length, hidden dimension size\u001b[39;00m\n\u001b[1;32m     31\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder(melspecs, melspecs_lengths)\n\u001b[0;32m---> 33\u001b[0m ctc_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                   \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmelspecs_lengths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentence_lengths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m ctc_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     40\u001b[0m enc_optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/torch/nn/modules/loss.py:1743\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_infinity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/speech/lib/python3.9/site-packages/torch/nn/functional.py:2599\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[1;32m   2593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2594\u001b[0m         ctc_loss,\n\u001b[1;32m   2595\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[1;32m   2596\u001b[0m         log_probs, targets, input_lengths, target_lengths,\n\u001b[1;32m   2597\u001b[0m         blank\u001b[38;5;241m=\u001b[39mblank, reduction\u001b[38;5;241m=\u001b[39mreduction, zero_infinity\u001b[38;5;241m=\u001b[39mzero_infinity\n\u001b[1;32m   2598\u001b[0m     )\n\u001b[0;32m-> 2599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_infinity\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "# model = Model(**model_params, vocab_size=vocab_size).to(device)\n",
    "EPOCHS = 1\n",
    "num_batches = len(train_loader)\n",
    "\n",
    "enc_optim = torch.optim.AdamW(model.encoder.parameters(), lr = 3e-4)\n",
    "loss_fn = nn.CTCLoss(blank = tokenizer.pad_token_id)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    kbar = pkbar.Kbar(target = num_batches, epoch = epoch, num_epochs=EPOCHS, width = 10, always_stateful=False)\n",
    "    \n",
    "    enc_optim.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        \n",
    "        # waveforms = batch['waveforms']\n",
    "        # waveforms_lengths = batch['waveforms_lengths']\n",
    "\n",
    "        sentences = batch['sentences'].to(device)\n",
    "        sentence_lengths = batch['sentence_lengths'].to(device)\n",
    "\n",
    "        melspecs = batch['melspecs'].to(device)\n",
    "        melspecs_lengths = batch['melspecs_lengths'].to(device)\n",
    "\n",
    "        melspecs = torch.transpose(melspecs, -1, -2) ## Changing to (batch, channel, time, n_mels) from (batch, channel, n_mels, time)\n",
    "\n",
    "        decoded = []\n",
    "\n",
    "        bsz, msl, hdz = melspecs.shape ##batch_size, max sequence length, hidden dimension size\n",
    "\n",
    "        encoder_outputs = model.encoder(melspecs, melspecs_lengths)\n",
    "\n",
    "        ctc_loss = loss_fn(log_probs = encoder_outputs.transpose(1, 0), \n",
    "                           targets = sentences, \n",
    "                           input_lengths = melspecs_lengths.type(torch.int32), \n",
    "                           target_lengths=sentence_lengths.type(torch.int32))\n",
    "\n",
    "        ctc_loss.backward()\n",
    "\n",
    "        enc_optim.step()\n",
    "        enc_optim.zero_grad(set_to_none=True)\n",
    "\n",
    "        kbar.update(idx, values = [(\"ctc_loss\", ctc_loss.detach().cpu().item())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ddf62-b2c9-4a22-bb96-6d59c818da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(tokenizer)\n",
    "BATCH_SIZE = 1\n",
    "train_loader = DataLoader(train_data, batch_size = BATCH_SIZE, collate_fn=collator, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ab1bf-ac78-4ff4-a612-49773a4c46f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'encoder_num_heads': 4,\n",
    "    'encoder_ffn_dim': 128,\n",
    "    'encoder_num_layers': 3,\n",
    "    'decoder_num_layers': 1,\n",
    "    'decoder_hidden_size': 128,\n",
    "    'padding_idx': tokenizer.pad_token_id,\n",
    "    'sos_token_id': tokenizer.bos_token_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d8260d-e352-43ae-8887-e5ccdc1fea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(**model_params, vocab_size=vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4eaac7-aa8e-47d4-b462-b96c4baf4a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "loss_fn = nn.CTCLoss(blank = tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01abc36-ab35-434c-9447-f75647921c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16 = True\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fe8e9-01e0-4416-9d45-40f1f8676e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_step(batch):\n",
    "\n",
    "#     for t in range(msl):                        \n",
    "\n",
    "#         outputs = F.glu(self.ffn(outputs))\n",
    "\n",
    "#         word = F.log_softmax(outputs, dim = -1) ## have to do log_softmax for CTC Loss\n",
    "\n",
    "#         decoder_inputs = outputs\n",
    "\n",
    "#         decoded.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b94dd-c3cb-4143-914c-f8ae6b15ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(loader, kbar):\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for idx, batch in enumerate(loader):\n",
    "        \n",
    "        waveforms = batch['waveforms']\n",
    "        waveforms_lengths = batch['waveforms_lengths']\n",
    "\n",
    "        sentences = batch['sentences'].to(device)\n",
    "        sentence_lengths = batch['sentence_lengths'].to(device)\n",
    "\n",
    "        melspecs = batch['melspecs'].to(device)\n",
    "        melspecs_lengths = batch['melspecs_lengths'].to(device)\n",
    "\n",
    "        melspecs = torch.transpose(melspecs, -1, -2) ## Changing to (batch, channel, time, n_mels) from (batch, channel, n_mels, time)\n",
    "        \n",
    "        if fp16:\n",
    "            \n",
    "            with autocast():\n",
    "\n",
    "                y = model.forward(melspecs, melspecs_lengths)\n",
    "\n",
    "                ## CTC loss requires int32 and (T, B, L) shape for log_probabilities from decoder\n",
    "\n",
    "                loss = loss_fn(log_probs = y.transpose(1, 0).type(torch.float32), \n",
    "                                  targets = sentences, \n",
    "                                  input_lengths = melspecs_lengths.type(torch.int32), \n",
    "                                  target_lengths=sentence_lengths.type(torch.int32))\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            y = model.forward(melspecs, melspecs_lengths)\n",
    "\n",
    "            ## CTC loss requires int32 and (T, B, L) shape for log_probabilities from decoder\n",
    "            \n",
    "            loss = F.ctc_loss(log_probs = y.transpose(1, 0), \n",
    "                              targets = sentences, \n",
    "                              input_lengths = melspecs_lengths.type(torch.int32), \n",
    "                              target_lengths=sentence_lengths.type(torch.int32))\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        kbar.update(idx, values = [(\"loss\", loss.detach().cpu().item())])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f74d704-04b1-42a2-8aeb-01c2de4f4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bf598f9-5363-4ca7-af0f-9397d302df3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m      5\u001b[0m     kbar \u001b[38;5;241m=\u001b[39m pkbar\u001b[38;5;241m.\u001b[39mKbar(target \u001b[38;5;241m=\u001b[39m num_batches, epoch \u001b[38;5;241m=\u001b[39m epoch, num_epochs\u001b[38;5;241m=\u001b[39mEPOCHS, width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, always_stateful\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mtrain_step\u001b[49m(train_loader, kbar)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_step' is not defined"
     ]
    }
   ],
   "source": [
    "num_batches = len(train_loader)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    kbar = pkbar.Kbar(target = num_batches, epoch = epoch, num_epochs=EPOCHS, width = 10, always_stateful=False)\n",
    "    \n",
    "    train_step(train_loader, kbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5252fc2b-a6af-4604-a895-2e24fe42e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [train_data.__getitem__(1), train_data.__getitem__(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761555e-600a-45ae-a882-350a42d84079",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e2065e-39ee-42b5-9c62-5e0a751e9330",
   "metadata": {},
   "outputs": [],
   "source": [
    "melspec = sample['melspec'].to(device)\n",
    "melspec_len = torch.Tensor([melspec.shape[-1]]).to(device)\n",
    "\n",
    "melspec= melspec.unsqueeze(0)\n",
    "melspec = melspec.transpose(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36919817-b4c0-4b67-b4ee-3321168af5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_preds = model(melspec, melspec_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc662b38-a204-4243-a526-0a906796ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa9a7f-2484-4522-8609-d85c8f1aeadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "topv, topi = y_preds.topk(1, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf1e7b-5925-42c8-956a-838b1daededc",
   "metadata": {},
   "outputs": [],
   "source": [
    "topi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf97fce2-1787-44fb-b78a-88a9828acf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "topi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882552f9-37fa-4b29-9b6b-4e5378deaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_preds = model(melspec, melspec_len)\n",
    "    y_preds = y_pred.argmax(dim = -1)\n",
    "    y_preds = torch.unique_consecutive(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a682b8-d49c-41bc-9332-690ca23932ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54994e67-e0f2-4a97-bdc6-360b6f020754",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(sample['sentence'], return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412e79f-b46e-4234-a639-fbd22e4f6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc153a-8cf8-45db-8c00-d77741f81584",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc60a6-6074-4e1d-bc5e-9437a5e75a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f670906-a7e9-43e3-8a9d-5bda8df3caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# melspec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cede0b2-7ad7-46b8-997f-6e151cfc6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# melspec_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae4825-6314-434b-96b7-831ff57721cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Speech",
   "language": "python",
   "name": "speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
